{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from IPython.core.debugger import set_trace\n",
    "# set device\n",
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['102世吉田日厚貫首', '1月15日：成人祭、新年祭', '1月3日：家運隆盛、商売繁盛祈願祭', '1月7日：七種粥神事', '21世紀COEプログラム']\n",
      "['the 102nd head priest, Nikko TOSHIDA', '15th January: Seijin-sai (Adult Festival), the New Year Festival', '3rd January: Prayer Festival for the prosperity of family fortunes and business', '7th January: Nanakusa-gayu shinji (a divine service for a rice porridge with seven spring herbs to insure health for the new year)', 'The 21st Century Center Of Excellence Program']\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "jp_sentences = []\n",
    "en_sentences = []\n",
    "with open('data/kyoto_lexicon.csv', 'r', encoding='utf-8') as file:\n",
    "    reader = csv.reader(file, delimiter=',')\n",
    "    # skip the header row\n",
    "    startLooking = False\n",
    "    for row in reader:\n",
    "        if startLooking:\n",
    "            jp_sentences.append(row[0])\n",
    "            en_sentences.append(row[1])\n",
    "        startLooking = True\n",
    "print(jp_sentences[:5])\n",
    "print(en_sentences[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# character-by-character prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[144, 90], [82, 44]]\n",
      "譴\n"
     ]
    }
   ],
   "source": [
    "class CharacterTable:\n",
    "    def __init__(self, charset):\n",
    "        self.charset = charset\n",
    "        self.charset = frozenset(self.charset)\n",
    "        self.charlist = ['<null>', '<sos>', '<eos>'] + list(self.charset)\n",
    "        self.vocab_size = len(self.charlist)\n",
    "    def encode(self, char):\n",
    "        '''convert from character to index\n",
    "        can process (nested) list of characters'''\n",
    "        if type(char) is type('asdf'):\n",
    "            # char is a string\n",
    "            return self.charlist.index(char)\n",
    "        else:\n",
    "            # char is a list of strings\n",
    "            return [self.encode(char) for char in char]\n",
    "    def decode(self, charInd):\n",
    "        '''convert from index to character\n",
    "        can process (nested) list of indices'''\n",
    "        if type(charInd) is type(22):\n",
    "            # charInd is an int\n",
    "            return self.charlist[charInd]\n",
    "        else:\n",
    "            # charInd is a list of ints\n",
    "            return [self.encode(charInd) for charInd in charInd]\n",
    "jp_chartable = CharacterTable(set(''.join(jp_sentences)))\n",
    "en_chartable = CharacterTable(set(''.join(en_sentences)))\n",
    "print(en_chartable.encode([['a', 'b'], ['c', 'd']]))\n",
    "print(jp_chartable.decode(1234))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# character-by-character prediction model\n",
    "class CharacterPredictor(nn.Module):\n",
    "    def __init__(self, chartable, embedding_dimensions=64, hidden_size=100):\n",
    "        super(CharacterPredictor, self).__init__()\n",
    "        # model constants\n",
    "        self.embedding_dimensions = embedding_dimensions\n",
    "        self.hidden_size = hidden_size\n",
    "        self.chartable = chartable\n",
    "        self.vocab_size = self.chartable.vocab_size\n",
    "        # model layers\n",
    "        self.embedding = nn.Embedding(self.vocab_size, embedding_dimensions)\n",
    "        self.RNN = nn.LSTM(\n",
    "            input_size=self.embedding_dimensions,\n",
    "            hidden_size=self.hidden_size, \n",
    "            batch_first=True\n",
    "        )\n",
    "        # linear layer for converting from hidden state to softmax\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(self.hidden_size, self.vocab_size),\n",
    "            nn.LogSoftmax(dim=-1)\n",
    "        )\n",
    "    \n",
    "    \n",
    "    def forward(self, padded_seq, lengths):\n",
    "        '''\n",
    "        predicts sequence of characters at every step\n",
    "        seq (batch, seq) padded tensor of character indices\n",
    "        returns (batch, seq, vocab) softmaxes\n",
    "        implicit teacher forcing by torch RNN\n",
    "        '''\n",
    "        padded_seq_embed = self.embedding(padded_seq) # (batch, seq, embed)\n",
    "        packed_seq_embed = torch.nn.utils.rnn.pad_packed_sequence(padded_seq_embed, lengths, batch_first=True)\n",
    "        batch_size = seq.shape[0]\n",
    "        hidden_states, (h_final, cell_final) = self.RNN(packed_seq_embed)\n",
    "        # hidden_states (seq, batch, hidden) hidden states\n",
    "        y_hat = self.linear(hidden_states)\n",
    "        # y_hat (seq, batch, vocab) softmaxes\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[1;32m<ipython-input-50-49b30accb1c5>\u001b[0m(23)\u001b[0;36mtrain_test\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m     20 \u001b[1;33m    \u001b[0mpadded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpadded\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mperm_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m     21 \u001b[1;33m    \u001b[0mset_trace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m     22 \u001b[1;33m    \u001b[1;31m### left off here. you need to pack the input before passing it through the lstm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m---> 23 \u001b[1;33m    \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m     24 \u001b[1;33m\u001b[0mtrain_test\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0men_sentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0men_chartable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "torch.Size([51982, 315])\n"
     ]
    },
    {
     "ename": "BdbQuit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBdbQuit\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-50-49b30accb1c5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;31m### left off here. you need to pack the input before passing it through the lstm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[0mtrain_test\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0men_sentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0men_chartable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-50-49b30accb1c5>\u001b[0m in \u001b[0;36mtrain_test\u001b[1;34m(sentences, chartable, train_test_split)\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0mset_trace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;31m### left off here. you need to pack the input before passing it through the lstm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m     \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[0mtrain_test\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0men_sentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0men_chartable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-50-49b30accb1c5>\u001b[0m in \u001b[0;36mtrain_test\u001b[1;34m(sentences, chartable, train_test_split)\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0mset_trace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;31m### left off here. you need to pack the input before passing it through the lstm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m     \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[0mtrain_test\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0men_sentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0men_chartable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mthun\\appdata\\local\\programs\\python\\python35\\lib\\bdb.py\u001b[0m in \u001b[0;36mtrace_dispatch\u001b[1;34m(self, frame, event, arg)\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;31m# None\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'line'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_line\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'call'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mthun\\appdata\\local\\programs\\python\\python35\\lib\\bdb.py\u001b[0m in \u001b[0;36mdispatch_line\u001b[1;34m(self, frame)\u001b[0m\n\u001b[0;32m     65\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop_here\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbreak_here\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muser_line\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquitting\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mraise\u001b[0m \u001b[0mBdbQuit\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrace_dispatch\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mBdbQuit\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# load data\n",
    "def group_by_length(lists):\n",
    "    '''2D list -> {length: 2D list of things with that length}\n",
    "    rows grouped by length under a dictionary from length to list of rows of that length'''\n",
    "    grouped_list = {}\n",
    "    for row in lists:\n",
    "        length = len(row)\n",
    "        if length in grouped_list:\n",
    "            grouped_list[length].append(row)\n",
    "        else:\n",
    "            grouped_list[length] = [row]\n",
    "    return grouped_list\n",
    "def train_test(sentences, chartable, train_test_split=.2):\n",
    "    def pad_sequence(sentences):\n",
    "        sentence_indices = [chartable.encode(list(sentence)) for sentence in sentences]\n",
    "        # list of list of indices\n",
    "        lengths = torch.LongTensor([len(sentence) for sentence in sentence_indices])\n",
    "        sentence_tensors = [torch.LongTensor(sentence).to(device) for sentence in  sentence_indices]\n",
    "        padded = torch.nn.utils.rnn.pad_sequence(sentence_tensors, batch_first=True)\n",
    "        lengths, perm_idx = lengths.sort(0, descending=True)\n",
    "        padded = padded[perm_idx]\n",
    "        return padded, lengths\n",
    "    length = len(sentences)\n",
    "    split = floor(length * train_test_split)\n",
    "    ### left off here. just call pad_sequence on split sentences \n",
    "    \n",
    "train_test(en_sentences, en_chartable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_char(chartable, ):\n",
    "    model = CharacterPredictor(chartable)\n",
    "    optimizer = torch.optim.SGD()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
