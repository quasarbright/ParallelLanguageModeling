{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import pdb\n",
    "# set device\n",
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['102世吉田日厚貫首', '1月15日：成人祭、新年祭', '1月3日：家運隆盛、商売繁盛祈願祭', '1月7日：七種粥神事', '21世紀COEプログラム']\n",
      "['the 102nd head priest, Nikko TOSHIDA', '15th January: Seijin-sai (Adult Festival), the New Year Festival', '3rd January: Prayer Festival for the prosperity of family fortunes and business', '7th January: Nanakusa-gayu shinji (a divine service for a rice porridge with seven spring herbs to insure health for the new year)', 'The 21st Century Center Of Excellence Program']\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "jp_sentences = []\n",
    "en_sentences = []\n",
    "with open('data/kyoto_lexicon.csv', 'r', encoding='utf-8') as file:\n",
    "    reader = csv.reader(file, delimiter=',')\n",
    "    # skip the header row\n",
    "    startLooking = False\n",
    "    for row in reader:\n",
    "        if startLooking:\n",
    "            jp_sentences.append(row[0])\n",
    "            en_sentences.append(row[1])\n",
    "        startLooking = True\n",
    "print(jp_sentences[:5])\n",
    "print(en_sentences[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# character-by-character prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[144, 90], [82, 44]]\n",
      "譴\n"
     ]
    }
   ],
   "source": [
    "class CharacterTable:\n",
    "    def __init__(self, charset):\n",
    "        self.charset = charset\n",
    "        self.charset = frozenset(self.charset)\n",
    "        self.charlist = ['<null>', '<sos>', '<eos>'] + list(self.charset)\n",
    "        self.vocab_size = len(self.charlist)\n",
    "    def encode(self, char):\n",
    "        '''convert from character to index\n",
    "        can process (nested) list of characters'''\n",
    "        if type(char) is type('asdf'):\n",
    "            # char is a string\n",
    "            return self.charlist.index(char)\n",
    "        else:\n",
    "            # char is a list of strings\n",
    "            return [self.encode(char) for char in char]\n",
    "    def decode(self, charInd):\n",
    "        '''convert from index to character\n",
    "        can process (nested) list of indices'''\n",
    "        if type(charInd) is type(22):\n",
    "            # charInd is an int\n",
    "            return self.charlist[charInd]\n",
    "        else:\n",
    "            # charInd is a list of ints\n",
    "            return [self.encode(charInd) for charInd in charInd]\n",
    "jp_chartable = CharacterTable(set(''.join(jp_sentences)))\n",
    "en_chartable = CharacterTable(set(''.join(en_sentences)))\n",
    "print(en_chartable.encode([['a', 'b'], ['c', 'd']]))\n",
    "print(jp_chartable.decode(1234))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# character-by-character prediction model\n",
    "class CharacterPredictor(nn.Module):\n",
    "    def __init__(self, chartable, embedding_dimensions=64, hidden_size=100):\n",
    "        super(CharacterPredictor, self).__init__()\n",
    "        # model constants\n",
    "        self.embedding_dimensions = embedding_dimensions\n",
    "        self.hidden_size = hidden_size\n",
    "        self.chartable = chartable\n",
    "        self.vocab_size = self.chartable.vocab_size\n",
    "        # model layers\n",
    "        self.embedding = nn.Embedding(self.vocab_size, embedding_dimensions)\n",
    "        self.RNN = nn.LSTM(\n",
    "            input_size=self.embedding_dimensions,\n",
    "            hidden_size=self.hidden_size, \n",
    "            batch_first=True\n",
    "        )\n",
    "        # linear layer for converting from hidden state to softmax\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(self.hidden_size, self.vocab_size),\n",
    "            nn.LogSoftmax(dim=-1)\n",
    "        )\n",
    "    \n",
    "    \n",
    "    def forward(self, seq):\n",
    "        '''\n",
    "        predicts sequence of characters at every step\n",
    "        seq (batch, seq) tensor of character indices\n",
    "        returns (batch, seq, vocab) softmaxes\n",
    "        implicit teacher forcing by torch RNN\n",
    "        '''\n",
    "        seq_embed = self.embedding(seq) # (batch, seq, embed)\n",
    "        batch_size = seq.shape[0]\n",
    "        hidden_states, (h_final, cell_final) = self.RNN(seq_embed)\n",
    "        # hidden_states (seq, batch, hidden) hidden states\n",
    "        y_hat = self.linear(hidden_states)\n",
    "        # y_hat (seq, batch, vocab) softmaxes\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the 102nd head priest, Nikko TOSHIDA\n",
      "tensor([[161,  99, 132,  55, 143,  28,  43,  38,  44,  55,  99, 132, 144,  44,\n",
      "          55, 154,  94, 124, 132,  35, 161,  63,  55, 121, 124,  98,  98,  34,\n",
      "          55,  22, 100, 164,  36,   4,  92, 110]], device='cuda:0') torch.Size([1, 36])\n",
      "tensor([[[-5.1961, -5.1094, -5.1551,  ..., -5.2134, -5.1618, -5.2853],\n",
      "         [-5.2293, -5.1681, -5.1038,  ..., -5.2491, -5.0742, -5.2203],\n",
      "         [-5.2933, -5.1750, -5.1183,  ..., -5.2007, -5.1836, -5.2660],\n",
      "         ...,\n",
      "         [-5.1902, -5.0303, -5.1560,  ..., -5.2124, -5.1696, -5.3023],\n",
      "         [-5.2850, -4.9637, -5.2178,  ..., -5.1580, -5.0941, -5.2161],\n",
      "         [-5.3089, -5.0646, -5.1772,  ..., -5.2496, -5.0917, -5.1905]]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward>) torch.Size([1, 36, 174])\n"
     ]
    }
   ],
   "source": [
    "# example usage of model\n",
    "model = CharacterPredictor(en_chartable).to(device)\n",
    "sentence = en_sentences[0]\n",
    "sentence_indices = [en_chartable.encode(list(sentence))]\n",
    "sentence_tensor = torch.LongTensor(sentence_indices).to(device)\n",
    "print(sentence)\n",
    "print(sentence_tensor, sentence_tensor.shape)\n",
    "print(model(sentence_tensor), model(sentence_tensor).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# figure out packing\n",
    "l = [\n",
    "    [1, 2, 3],\n",
    "    [4, 5, 0],\n",
    "    [6, 1, 0]# not actually length 1, \n",
    "]\n",
    "padded = torch.tensor(l)\n",
    "lengths = torch.tensor([3,2,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[161, 98, 131, 54, 142, 27, 42, 37, 43, 54, 98, 131, 143, 43, 54, 154, 93, 123, 131, 34, 161, 62, 54, 120, 123, 97, 97, 33, 54, 21, 99, 164, 35, 1, 91, 109]\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "def group_by_length(lists):\n",
    "    '''2D list -> {length: 2D list of things with that length}\n",
    "    rows grouped by length under a dictionary from length to list of rows of that length'''\n",
    "    grouped_list = {}\n",
    "    for row in lists:\n",
    "        length = len(row)\n",
    "        if length in grouped_list:\n",
    "            grouped_list[length].append(row)\n",
    "        else:\n",
    "            grouped_list[length] = [row]\n",
    "    return grouped_list\n",
    "def train_test(sentences, chartable, train_test_split=.2):\n",
    "    sentence_indices = [chartable.encode(list(sentence)) for sentence in sentences]\n",
    "    # list of list of indices\n",
    "    lengths = torch.LongTensor([len(sentence) for sentence in sentence_indices])\n",
    "    sentence_tensors = [torch.LongTensor(sentence).to(device) for sentence in  sentence_indices]\n",
    "    padded = torch.nn.utils.rnn.pad_sequence(sentence_tensors, batch_first=True)\n",
    "    lengths, perm_idx = lengths.sort(0, descending=True)\n",
    "    padded = padded[perm_idx]\n",
    "    ### left off here. you need to pack the input before passing it through the lstm\n",
    "    lengths = sentence_tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_char(chartable, ):\n",
    "    model = CharacterPredictor(chartable)\n",
    "    optimizer = torch.optim.SGD()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
