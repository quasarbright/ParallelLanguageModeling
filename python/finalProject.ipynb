{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from IPython.core.debugger import set_trace\n",
    "# set device\n",
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['102世吉田日厚貫首', '1月15日：成人祭、新年祭', '1月3日：家運隆盛、商売繁盛祈願祭', '1月7日：七種粥神事', '21世紀COEプログラム']\n",
      "['the 102nd head priest, Nikko TOSHIDA', '15th January: Seijin-sai (Adult Festival), the New Year Festival', '3rd January: Prayer Festival for the prosperity of family fortunes and business', '7th January: Nanakusa-gayu shinji (a divine service for a rice porridge with seven spring herbs to insure health for the new year)', 'The 21st Century Center Of Excellence Program']\n",
      "51982\n",
      "51982\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "jp_sentences = []\n",
    "en_sentences = []\n",
    "with open('data/kyoto_lexicon.csv', 'r', encoding='utf-8') as file:\n",
    "    reader = csv.reader(file, delimiter=',')\n",
    "    # skip the header row\n",
    "    startLooking = False\n",
    "    for row in reader:\n",
    "        if startLooking:\n",
    "            jp_sentences.append(row[0])\n",
    "            en_sentences.append(row[1])\n",
    "        startLooking = True\n",
    "print(jp_sentences[:5])\n",
    "print(en_sentences[:5])\n",
    "print(len(jp_sentences))\n",
    "print(len(en_sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# character-by-character prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[102, 78], [32, 47]]\n",
      "値\n",
      "3911 173\n"
     ]
    }
   ],
   "source": [
    "# encoding and decoding characters\n",
    "class CharacterTable:\n",
    "    def __init__(self, charset):\n",
    "        self.charset = charset\n",
    "        self.charset = frozenset(self.charset)\n",
    "        self.charlist = ['<null>', '<sos>'] + list(self.charset)\n",
    "        # it is important that null is at index 0 since padding fills with zeroes\n",
    "        self.vocab_size = len(self.charlist)\n",
    "    def encode(self, char):\n",
    "        '''convert from character to index\n",
    "        can process (nested) list of characters'''\n",
    "        if type(char) is type('asdf'):\n",
    "            # char is a string\n",
    "            return self.charlist.index(char)\n",
    "        else:\n",
    "            # char is a list of strings\n",
    "            return [self.encode(char) for char in char]\n",
    "    def decode(self, charInd):\n",
    "        '''convert from index to character\n",
    "        can process (nested) list of indices'''\n",
    "        if type(charInd) is type(22):\n",
    "            # charInd is an int\n",
    "            return self.charlist[charInd]\n",
    "        else:\n",
    "            # charInd is a list of ints\n",
    "            return [self.decode(charInd) for charInd in charInd]\n",
    "jp_chartable = CharacterTable(set(''.join(jp_sentences)))\n",
    "en_chartable = CharacterTable(set(''.join(en_sentences)))\n",
    "print(en_chartable.encode([['a', 'b'], ['c', 'd']]))\n",
    "print(jp_chartable.decode(1234))\n",
    "print(jp_chartable.vocab_size, en_chartable.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequence prediction model\n",
    "class Predictor(nn.Module):\n",
    "    def __init__(self, table, embedding_dimensions=64, hidden_size=100):\n",
    "        super(Predictor, self).__init__()\n",
    "        # model constants\n",
    "        self.embedding_dimensions = embedding_dimensions\n",
    "        self.hidden_size = hidden_size\n",
    "        self.table = table\n",
    "        self.vocab_size = self.table.vocab_size\n",
    "        # model layers\n",
    "        self.embedding = nn.Embedding(self.vocab_size, embedding_dimensions)\n",
    "        self.RNN = nn.LSTM(\n",
    "            input_size=self.embedding_dimensions,\n",
    "            hidden_size=self.hidden_size, \n",
    "            batch_first=True\n",
    "        )\n",
    "        # linear layer for converting from hidden state to softmax\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(self.hidden_size, self.vocab_size),\n",
    "            nn.LogSoftmax(dim=-1)\n",
    "        )\n",
    "    \n",
    "    \n",
    "    def forward(self, padded_seq, lengths):\n",
    "        '''\n",
    "        predicts sequence of characters at every step\n",
    "        seq (batch, seq) padded tensor of character indices\n",
    "        returns (batch, seq, vocab) softmaxes\n",
    "        implicit teacher forcing by torch RNN\n",
    "        '''\n",
    "        seq_len = padded_seq.shape[1]\n",
    "        padded_seq_embed = self.embedding(padded_seq) # (batch, seq, embed)\n",
    "        packed_seq_embed = torch.nn.utils.rnn.pack_padded_sequence(padded_seq_embed, lengths, batch_first=True)\n",
    "        packed_hidden_states, (h_final, cell_final) = self.RNN(packed_seq_embed)\n",
    "        padded_hidden_states, input_sizes = torch.nn.utils.rnn.pad_packed_sequence(packed_hidden_states, batch_first=True, total_length=seq_len)\n",
    "        # hidden_states (batch, seq, hidden) hidden states\n",
    "        y_hat = self.linear(padded_hidden_states)\n",
    "        # y_hat (batch, seq, vocab) softmaxes\n",
    "        return y_hat\n",
    "    \n",
    "    \n",
    "    def predict(self, padded_seq, lengths):\n",
    "        pred = self.forward(padded_seq, lengths)\n",
    "        # (batch, seq, vocab)\n",
    "        maxInds = pred.max(2)[1]\n",
    "        # (batch, seq)\n",
    "        return pred, maxInds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "def padded_train_test(sentences, table, train_test_split=.2, batch_size=500, word=False):\n",
    "    '''\n",
    "    small train_test_split means mostly train data\n",
    "    ['hello world', ...] or [['hello', 'world',...],...], table, train_test_split -> (train data, test data) padded tensor dataloaders\n",
    "    small train_test_split means mostly train data\n",
    "    output \"shapes\" (train_size, maxlen), (test_size, maxlen) with given batch size\n",
    "    '''\n",
    "    \n",
    "    def encode_sequence(sentences, ans):\n",
    "        # ans is whether this is the input sequence or the true sequence\n",
    "        # ans=True means don't append an <sos>\n",
    "        # ans=False means append an <sos> and remove the last\n",
    "        sentence_indices = []\n",
    "        for sentence in sentences:\n",
    "            if word:\n",
    "                encoded = table.encode(sentence)\n",
    "            else:\n",
    "                encoded = table.encode(list(sentence))\n",
    "            if not ans:\n",
    "                # add sos and remove last\n",
    "                # this is an input sequence\n",
    "                encoded = [table.encode('<sos>')] + encoded[:-1]\n",
    "            sentence_indices.append(encoded)\n",
    "        return sentence_indices\n",
    "    def pad_sequence(sentences, ans):\n",
    "        # ans is whether this is the input sequence or the true sequence\n",
    "        # ans=True means don't append an <sos>\n",
    "        # ans=False means append an <sos> and remove the last\n",
    "        '''\n",
    "        ['hello world', ...] or [['hello', 'world',...],...] -> (padded long tensor, lengths tensor)\n",
    "        tensors are padded and sorted \n",
    "        '''\n",
    "        sentence_indices = encode_sequence(sentences, ans)\n",
    "        # list of list of indices\n",
    "        lengths = torch.LongTensor([len(sentence) for sentence in sentence_indices])\n",
    "        sentence_tensors = [torch.LongTensor(sentence).to(device) for sentence in  sentence_indices]\n",
    "        padded = torch.nn.utils.rnn.pad_sequence(sentence_tensors, batch_first=True)\n",
    "        lengths, perm_idx = lengths.sort(0, descending=True)\n",
    "        # perm_idx is the permutation of sentence indices as sorted by length\n",
    "        padded = padded[perm_idx]\n",
    "        return padded, lengths\n",
    "    \n",
    "    length = len(sentences)\n",
    "    # the index to separate train from test\n",
    "    split = int(length * train_test_split)\n",
    "    \n",
    "    # shuffle before splitting so test doesn't just get the alphabetically sooner sentences\n",
    "    sentences = random.sample(sentences, length)\n",
    "    \n",
    "    train_sentences = sentences[split:]\n",
    "    test_sentences = sentences[:split]\n",
    "    \n",
    "    # the input sequences (with sos and removed last)\n",
    "    padded_train_in = pad_sequence(train_sentences, False)\n",
    "    padded_test_in = pad_sequence(test_sentences, False)\n",
    "    # the output sequences (with no sos)\n",
    "    padded_train_true = pad_sequence(train_sentences, True)\n",
    "    padded_test_true = pad_sequence(test_sentences, True)\n",
    "    \n",
    "    padded_trainset = torch.utils.data.TensorDataset(*padded_train_in, *padded_train_true)\n",
    "    padded_testset = torch.utils.data.TensorDataset(*padded_test_in, *padded_test_true)\n",
    "    \n",
    "    padded_trainloader = torch.utils.data.DataLoader(padded_trainset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    padded_testloader = torch.utils.data.DataLoader(padded_testset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    # shuffle must be false to maintain sorting by length\n",
    "    \n",
    "    return padded_trainloader, padded_testloader\n",
    "padded_en_trainloader, padded_en_testloader = padded_train_test(en_sentences, en_chartable)\n",
    "padded_jp_trainloader, padded_jp_testloader = padded_train_test(jp_sentences, jp_chartable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(trainloader, table, lr=.1, epochs=200):\n",
    "    model = Predictor(table).to(device)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.NLLLoss()\n",
    "    losses = []\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        num_losses = 0\n",
    "        for index, data in enumerate(trainloader, 0):\n",
    "            model.zero_grad()\n",
    "            padded_seq_in, lengths_in, padded_seq_true, lengths_true = data\n",
    "            pred = model(padded_seq_in, lengths_in)\n",
    "            \n",
    "            batch_size = padded_seq_true.shape[0]\n",
    "            maxlen = padded_seq_true.shape[1]\n",
    "            vocab_size = pred.shape[-1]\n",
    "            padded_seq_true_flat = padded_seq_true.view(batch_size*maxlen)\n",
    "            pred_flat = pred.contiguous().view(batch_size*maxlen, vocab_size)\n",
    "            \n",
    "            loss = loss_fn(pred_flat, padded_seq_true_flat)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.data\n",
    "            num_losses += 1\n",
    "        avg_loss = total_loss / num_losses\n",
    "        losses.append(avg_loss)\n",
    "        if (epoch + 1) % (epochs // 10) == 0:\n",
    "            print('loss at epoch {}: {}'.format(epoch+1, avg_loss))\n",
    "    print('final loss after {} epochs: {}'.format(epochs, losses[-1]))\n",
    "    return model, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save and load model\n",
    "def get_state_path(name):\n",
    "    return 'states/{}.pt'.format(name)\n",
    "def save_model(model, name):\n",
    "    torch.save(model, get_state_path(name))\n",
    "def load_model(model, name):\n",
    "    '''loads state dict into given model and returns it'''\n",
    "    model = torch.load(get_state_path(name))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jp training\n",
      "loss at epoch 20: 0.814089834690094\n",
      "loss at epoch 40: 0.7519795894622803\n",
      "loss at epoch 60: 0.7228744029998779\n",
      "loss at epoch 80: 0.709435760974884\n",
      "loss at epoch 100: 0.7016097903251648\n",
      "loss at epoch 120: 0.6961705684661865\n",
      "loss at epoch 140: 0.691956102848053\n",
      "loss at epoch 160: 0.6884689927101135\n",
      "loss at epoch 180: 0.6854720115661621\n",
      "loss at epoch 200: 0.682823121547699\n",
      "final loss after 200 epochs: 0.682823121547699\n",
      "en training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\mthun\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\torch\\serialization.py:251: UserWarning: Couldn't retrieve source code for container of type Predictor. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at epoch 20: 0.25185489654541016\n",
      "loss at epoch 40: 0.23443341255187988\n",
      "loss at epoch 60: 0.2246248722076416\n",
      "loss at epoch 80: 0.21615473926067352\n",
      "loss at epoch 100: 0.20942629873752594\n",
      "loss at epoch 120: 0.2041759341955185\n",
      "loss at epoch 140: 0.19995242357254028\n",
      "loss at epoch 160: 0.19644670188426971\n",
      "loss at epoch 180: 0.19343675673007965\n",
      "loss at epoch 200: 0.1907539814710617\n",
      "final loss after 200 epochs: 0.1907539814710617\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Predictor(\n",
       "   (embedding): Embedding(3911, 64)\n",
       "   (RNN): LSTM(64, 100, batch_first=True)\n",
       "   (linear): Sequential(\n",
       "     (0): Linear(in_features=100, out_features=3911, bias=True)\n",
       "     (1): LogSoftmax()\n",
       "   )\n",
       " ), Predictor(\n",
       "   (embedding): Embedding(173, 64)\n",
       "   (RNN): LSTM(64, 100, batch_first=True)\n",
       "   (linear): Sequential(\n",
       "     (0): Linear(in_features=100, out_features=173, bias=True)\n",
       "     (1): LogSoftmax()\n",
       "   )\n",
       " ))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def initialize_models(should_train=True):\n",
    "    global jp_model, en_model\n",
    "    jp_model = Predictor(jp_chartable).to(device)\n",
    "    jp_losses = None\n",
    "    en_model = Predictor(en_chartable).to(device)\n",
    "    en_losses = None\n",
    "    if should_train:\n",
    "        print('jp training')\n",
    "        jp_model, jp_losses = train_model(padded_jp_trainloader, jp_chartable)\n",
    "        save_model(jp_model, 'jp_char_model')\n",
    "        print('en training')\n",
    "        en_model, en_losses = train_model(padded_en_trainloader, en_chartable)\n",
    "        save_model(en_model, 'en_char_model')\n",
    "    else:\n",
    "        jp_model = load_model(jp_model, 'jp_char_model')\n",
    "        en_model = load_model(en_model, 'en_char_model')\n",
    "    return jp_model, en_model\n",
    "initialize_models(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics\n",
    "def perplexity_metric(pred, actual):\n",
    "    '''\n",
    "    pred (batch, seq, vocab) logsoftmax\n",
    "    actual (batch, seq) longs\n",
    "    geometric mean of product of p(next word | previous words) for whole sentence\n",
    "    average (arithmetic mean) by batch\n",
    "    '''\n",
    "    batch_size, seq_len, vocab_size = pred.shape\n",
    "    pred = pred.cpu()\n",
    "    pred = torch.exp(pred)\n",
    "    geo_means = [] # probabilities of correct characters\n",
    "    for i in range(batch_size):\n",
    "        product = 1\n",
    "        num_factors = 0\n",
    "        curr_pred = pred[i]\n",
    "        curr_actual = actual[i]\n",
    "        for t in range(seq_len):\n",
    "            trueInd = curr_actual[t].item()\n",
    "            # the character index at this timestep\n",
    "            if trueInd != 0:\n",
    "                # we don't care how well it predicts nulls\n",
    "                predSoftmax = curr_pred[t]\n",
    "                confidence = predSoftmax[trueInd].item()\n",
    "                product *= confidence\n",
    "                num_factors += 1\n",
    "        geo_means.append(product ** (1/num_factors))\n",
    "    return sum(geo_means) / len(geo_means)\n",
    "def print_metrics(model, name, testloader, word=False):\n",
    "    loss_fn = nn.NLLLoss()\n",
    "    losses = []\n",
    "    sentence_accuracies = []\n",
    "    character_accuracies = []\n",
    "    perplexities = []\n",
    "    for index, data in enumerate(testloader, 0):\n",
    "        padded_seq_in, lengths_in, padded_seq, lengths = data\n",
    "        pred, maxInds = model.predict(padded_seq, lengths)\n",
    "        \n",
    "        perplexity = perplexity_metric(pred, padded_seq)\n",
    "        \n",
    "        batch_size = padded_seq.shape[0]\n",
    "        maxlen = padded_seq.shape[1]\n",
    "        vocab_size = pred.shape[-1]\n",
    "        \n",
    "        padded_seq_flat = padded_seq.view(batch_size*maxlen)\n",
    "        pred_flat = pred.contiguous().view(batch_size*maxlen, vocab_size)\n",
    "        loss = loss_fn(pred_flat, padded_seq_flat).item()\n",
    "        \n",
    "        correct_characters = torch.sum(maxInds == padded_seq).item()\n",
    "        total_characters = batch_size*maxlen\n",
    "        correct_sentences = 0\n",
    "        total_sentences = batch_size\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            if torch.all(maxInds[i] == padded_seq[i]):\n",
    "                correct_sentences += 1\n",
    "        sentence_accuracy = correct_sentences / total_sentences\n",
    "        character_accuracy = correct_characters / total_characters\n",
    "        \n",
    "        losses.append(loss)\n",
    "        sentence_accuracies.append(sentence_accuracy)\n",
    "        character_accuracies.append(character_accuracy)\n",
    "        perplexities.append(perplexity)\n",
    "    loss_avg = sum(losses) / len(losses)\n",
    "    sentence_accuracy_avg = sum(sentence_accuracies) / len(sentence_accuracies)\n",
    "    character_accuracy_avg = sum(character_accuracies) / len(character_accuracies)\n",
    "    perplexity_avg = sum(perplexities) / len(perplexities)\n",
    "    if word:\n",
    "        print('model: {}\\n\\tvalidation loss: {}\\n\\tsentence accuracy: {}\\n\\tword accuracy: {}\\n\\tperplexity: {}'.format(name, loss_avg, sentence_accuracy_avg, character_accuracy_avg, perplexity_avg))\n",
    "    else:\n",
    "        print('model: {}\\n\\tvalidation loss: {}\\n\\tsentence accuracy: {}\\n\\tcharacter accuracy: {}\\n\\tperplexity: {}'.format(name, loss_avg, sentence_accuracy_avg, character_accuracy_avg, perplexity_avg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# english word-to-word\n",
    "since the japanese model had to learn a mixture of character prediction and word prediction at the same time, let's see how the english model predicts words, and compare it to the japanese character predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mthun\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', '102nd', 'head', 'priest', ',', 'Nikko', 'TOSHIDA']\n"
     ]
    }
   ],
   "source": [
    "# tokenize sentences\n",
    "tokenized_sentences = []\n",
    "for sentence in en_sentences:\n",
    "    tokenized = nltk.word_tokenize(sentence)\n",
    "    if len(tokenized) > 0:\n",
    "        tokenized_sentences.append(tokenized)\n",
    "print(tokenized_sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43216"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordlist = []\n",
    "for sentence in tokenized_sentences:\n",
    "    for word in sentence:\n",
    "        wordlist.append(word)\n",
    "wordset = set(wordlist)\n",
    "len(wordset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### that's way too many words!\n",
    "let's limit the vocab size to 4000 to make the complexity theoretically similar to the japanese model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "of\t6995\n",
      "(\t6793\n",
      ")\t6769\n",
      "the\t5777\n",
      ",\t3457\n",
      "no\t2899\n",
      "a\t2872\n",
      "Temple\t1617\n",
      "and\t1278\n",
      "in\t1175\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_vocab_size = 4000\n",
    "# word -> frequency\n",
    "counts = {}\n",
    "for word in wordlist:\n",
    "    if word in counts:\n",
    "        counts[word] += 1\n",
    "    else:\n",
    "        counts[word] = 1\n",
    "sorted_wordset = sorted(list(wordset), key=lambda word: counts[word], reverse=True)\n",
    "for word in sorted_wordset[:10]:\n",
    "    print(word, counts[word], sep='\\t')\n",
    "vocab = set([])\n",
    "for word in sorted_wordset:\n",
    "    if len(vocab) < max_vocab_size:\n",
    "        vocab.add(word)\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Junction\n",
      "<unk>\n",
      "4003\n"
     ]
    }
   ],
   "source": [
    "# word encoding and decoding\n",
    "class WordTable:\n",
    "    def __init__(self, wordset):\n",
    "        self.wordset = frozenset(wordset)\n",
    "        self.wordlist = ['<null>', '<sos>', '<unk>'] + list(wordset)\n",
    "        self.vocab_size = len(self.wordlist)\n",
    "        \n",
    "        \n",
    "    def encode(self, word):\n",
    "        '''\n",
    "        expects word string or possibly nested list of word strings\n",
    "        unks out-of-vocab words\n",
    "        word(s) -> indices\n",
    "        '''\n",
    "        if type(word) == type('asdf'):\n",
    "            if word in self.wordlist:\n",
    "                return self.wordlist.index(word)\n",
    "            else:\n",
    "                # encode out-of-vocab words with unk\n",
    "                return self.wordlist.index('<unk>')\n",
    "        else:\n",
    "            words = word\n",
    "            return [self.encode(word) for word in words]\n",
    "        \n",
    "        \n",
    "    def decode(self, wordInd):\n",
    "        '''\n",
    "        expects wordInd index or possibly nested list of word indices\n",
    "        '''\n",
    "        if type(wordInd) == type(123):\n",
    "            return self.wordlist[wordInd]\n",
    "        else:\n",
    "            wordInds = wordInd\n",
    "            return [self.decode(wordInd) for wordInd in wordInds]\n",
    "wordtable = WordTable(vocab)\n",
    "print(wordtable.decode(200))\n",
    "print(wordtable.decode(wordtable.encode('why relu works')))\n",
    "print(wordtable.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "padded_word_trainloader, padded_word_testloader = padded_train_test(tokenized_sentences, wordtable, word=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training english word model\n",
      "loss at epoch 20: 0.327519029378891\n",
      "loss at epoch 40: 0.2957797348499298\n",
      "loss at epoch 60: 0.28763270378112793\n",
      "loss at epoch 80: 0.2824614942073822\n",
      "loss at epoch 100: 0.27881553769111633\n",
      "loss at epoch 120: 0.2760738134384155\n",
      "loss at epoch 140: 0.27389097213745117\n",
      "loss at epoch 160: 0.27206066250801086\n",
      "loss at epoch 180: 0.2704685628414154\n",
      "loss at epoch 200: 0.2690542936325073\n",
      "final loss after 200 epochs: 0.2690542936325073\n"
     ]
    }
   ],
   "source": [
    "print('training english word model')\n",
    "word_model, word_model_losses = train_model(padded_word_trainloader, wordtable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: jp character predictor\n",
      "\tvalidation loss: 1.0359487959316798\n",
      "\tsentence accuracy: 0.0\n",
      "\tcharacter accuracy: 0.8824169164169166\n",
      "\tperplexity: 0.000256158712471985\n",
      "model: en character predictor\n",
      "\tvalidation loss: 0.38650648295879364\n",
      "\tsentence accuracy: 0.0\n",
      "\tcharacter accuracy: 0.9165669905904281\n",
      "\tperplexity: 0.00980208015188648\n",
      "model: english word to word\n",
      "\tvalidation loss: 0.5625579747415724\n",
      "\tsentence accuracy: 0.0\n",
      "\tword accuracy: 0.9287701863354039\n",
      "\tperplexity: 0.001651558462524638\n"
     ]
    }
   ],
   "source": [
    "print_metrics(jp_model, 'jp character predictor', padded_jp_testloader)\n",
    "print_metrics(en_model, 'en character predictor', padded_en_testloader)\n",
    "print_metrics(word_model, 'english word to word', padded_word_testloader, word=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Six', '<unk>', 'of', 'Takashima', ',', '<unk>', ',', '<unk>', ',', '<unk>', ',', '<unk>', 'and', 'Aichi', '<null>', '<null>', '<null>', '<null>', '<null>', '<null>', '<null>', '<null>', '<null>', '<null>', '<null>', '<null>', '<null>', '<null>', '<null>', '<null>', '<null>', '<null>', '<null>', '<null>', '<null>', '<null>', '<null>', '<null>', '<null>', '<null>', '<null>', '<null>', '<null>', '<null>', '<null>', '<null>']\n",
      "['<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<null>', '<null>', '<null>', '<null>', '<null>', '<null>', '<null>', '<null>', '<null>', '<null>', '<null>', '<null>', '<null>', '<null>', '<null>', '<null>', '<null>', '<null>', '<null>', '<null>', '<null>', '<null>', '<null>', '<null>', '<null>', '<null>', '<null>', '<null>', '<null>', '<null>', '<null>', '<null>']\n"
     ]
    }
   ],
   "source": [
    "def predict_sentence(model, table, loader):\n",
    "    seq_in, len_in, seq_true, len_true = iter(loader).next()\n",
    "    pred, maxInds = model.predict(seq_in, len_in)\n",
    "    trues = table.decode(seq_true[200:201].cpu().numpy().tolist())\n",
    "    preds = table.decode(maxInds[200:201].cpu().detach().numpy().tolist())\n",
    "    zipped = list(zip(trues, preds))\n",
    "    for t, p in zipped:\n",
    "        print(t)\n",
    "        print(p)\n",
    "predict_sentence(word_model, wordtable, padded_word_testloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
