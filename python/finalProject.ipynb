{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from IPython.core.debugger import set_trace\n",
    "# set device\n",
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['102世吉田日厚貫首', '1月15日：成人祭、新年祭', '1月3日：家運隆盛、商売繁盛祈願祭', '1月7日：七種粥神事', '21世紀COEプログラム']\n",
      "['the 102nd head priest, Nikko TOSHIDA', '15th January: Seijin-sai (Adult Festival), the New Year Festival', '3rd January: Prayer Festival for the prosperity of family fortunes and business', '7th January: Nanakusa-gayu shinji (a divine service for a rice porridge with seven spring herbs to insure health for the new year)', 'The 21st Century Center Of Excellence Program']\n",
      "51982\n",
      "51982\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "jp_sentences = []\n",
    "en_sentences = []\n",
    "with open('data/kyoto_lexicon.csv', 'r', encoding='utf-8') as file:\n",
    "    reader = csv.reader(file, delimiter=',')\n",
    "    # skip the header row\n",
    "    startLooking = False\n",
    "    for row in reader:\n",
    "        if startLooking:\n",
    "            jp_sentences.append(row[0])\n",
    "            en_sentences.append(row[1])\n",
    "        startLooking = True\n",
    "print(jp_sentences[:5])\n",
    "print(en_sentences[:5])\n",
    "print(len(jp_sentences))\n",
    "print(len(en_sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# character-by-character prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[27, 46], [110, 10]]\n",
      "弼\n"
     ]
    }
   ],
   "source": [
    "# encoding and decoding characters\n",
    "class CharacterTable:\n",
    "    def __init__(self, charset):\n",
    "        self.charset = charset\n",
    "        self.charset = frozenset(self.charset)\n",
    "        self.charlist = ['<null>', '<sos>', '<eos>'] + list(self.charset)\n",
    "        # it is important that null is at index 0 since padding fills with zeroes\n",
    "        self.vocab_size = len(self.charlist)\n",
    "    def encode(self, char):\n",
    "        '''convert from character to index\n",
    "        can process (nested) list of characters'''\n",
    "        if type(char) is type('asdf'):\n",
    "            # char is a string\n",
    "            return self.charlist.index(char)\n",
    "        else:\n",
    "            # char is a list of strings\n",
    "            return [self.encode(char) for char in char]\n",
    "    def decode(self, charInd):\n",
    "        '''convert from index to character\n",
    "        can process (nested) list of indices'''\n",
    "        if type(charInd) is type(22):\n",
    "            # charInd is an int\n",
    "            return self.charlist[charInd]\n",
    "        else:\n",
    "            # charInd is a list of ints\n",
    "            return [self.encode(charInd) for charInd in charInd]\n",
    "jp_chartable = CharacterTable(set(''.join(jp_sentences)))\n",
    "en_chartable = CharacterTable(set(''.join(en_sentences)))\n",
    "print(en_chartable.encode([['a', 'b'], ['c', 'd']]))\n",
    "print(jp_chartable.decode(1234))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# character-by-character prediction model\n",
    "class CharacterPredictor(nn.Module):\n",
    "    def __init__(self, chartable, embedding_dimensions=64, hidden_size=100):\n",
    "        super(CharacterPredictor, self).__init__()\n",
    "        # model constants\n",
    "        self.embedding_dimensions = embedding_dimensions\n",
    "        self.hidden_size = hidden_size\n",
    "        self.chartable = chartable\n",
    "        self.vocab_size = self.chartable.vocab_size\n",
    "        # model layers\n",
    "        self.embedding = nn.Embedding(self.vocab_size, embedding_dimensions)\n",
    "        self.RNN = nn.LSTM(\n",
    "            input_size=self.embedding_dimensions,\n",
    "            hidden_size=self.hidden_size, \n",
    "            batch_first=True\n",
    "        )\n",
    "        # linear layer for converting from hidden state to softmax\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(self.hidden_size, self.vocab_size),\n",
    "            nn.LogSoftmax(dim=-1)\n",
    "        )\n",
    "    \n",
    "    \n",
    "    def forward(self, padded_seq, lengths):\n",
    "        '''\n",
    "        predicts sequence of characters at every step\n",
    "        seq (batch, seq) padded tensor of character indices\n",
    "        returns (batch, seq, vocab) softmaxes\n",
    "        implicit teacher forcing by torch RNN\n",
    "        '''\n",
    "        seq_len = padded_seq.shape[1]\n",
    "        padded_seq_embed = self.embedding(padded_seq) # (batch, seq, embed)\n",
    "        packed_seq_embed = torch.nn.utils.rnn.pack_padded_sequence(padded_seq_embed, lengths, batch_first=True)\n",
    "        packed_hidden_states, (h_final, cell_final) = self.RNN(packed_seq_embed)\n",
    "        padded_hidden_states, input_sizes = torch.nn.utils.rnn.pad_packed_sequence(packed_hidden_states, batch_first=True, total_length=seq_len)\n",
    "        # hidden_states (batch, seq, hidden) hidden states\n",
    "        y_hat = self.linear(padded_hidden_states)\n",
    "        # y_hat (batch, seq, vocab) softmaxes\n",
    "        return y_hat\n",
    "    \n",
    "    \n",
    "    def predict(self, padded_seq, lengths):\n",
    "        pred = self.forward(padded_seq, lengths)\n",
    "        # (batch, seq, vocab)\n",
    "        maxInds = pred.max(2)[1]\n",
    "        # (batch, seq)\n",
    "        return pred, maxInds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "def padded_train_test(sentences, chartable, train_test_split=.2, batch_size=500):\n",
    "    '''\n",
    "    small train_test_split means mostly train data\n",
    "    ['hello world', ...], chartable, train_test_split -> (train data, test data) padded long tensors of character indices\n",
    "    small train_test_split means mostly train data\n",
    "    output shapes (train_size, maxlen), (test_size, maxlen)\n",
    "    '''\n",
    "    def pad_sequence(sentences):\n",
    "        '''\n",
    "        ['hello world', ...] -> train dataloader, test data loader\n",
    "        outputs 2 dataloaders containing (seqs, lens)\n",
    "        tensors are padded and sorted \n",
    "        '''\n",
    "        sentence_indices = [chartable.encode(list(sentence)) for sentence in sentences]\n",
    "        # list of list of indices\n",
    "        lengths = torch.LongTensor([len(sentence) for sentence in sentence_indices])\n",
    "        sentence_tensors = [torch.LongTensor(sentence).to(device) for sentence in  sentence_indices]\n",
    "        padded = torch.nn.utils.rnn.pad_sequence(sentence_tensors, batch_first=True)\n",
    "        lengths, perm_idx = lengths.sort(0, descending=True)\n",
    "        # perm_idx is the permutation of sentence indices as sorted by length\n",
    "        padded = padded[perm_idx]\n",
    "        return padded, lengths\n",
    "    \n",
    "    length = len(sentences)\n",
    "    # the index to separate train from test\n",
    "    split = int(length * train_test_split)\n",
    "    \n",
    "    # shuffle before splitting so test doesn't just get the alphabetically sooner sentences\n",
    "    sentences = random.sample(sentences, length)\n",
    "    \n",
    "    train_sentences = sentences[split:]\n",
    "    test_sentences = sentences[:split]\n",
    "    \n",
    "    padded_train = pad_sequence(train_sentences)\n",
    "    padded_test = pad_sequence(test_sentences)\n",
    "    \n",
    "    padded_trainset = torch.utils.data.TensorDataset(*padded_train)\n",
    "    padded_testset = torch.utils.data.TensorDataset(*padded_test)\n",
    "    \n",
    "    padded_trainloader = torch.utils.data.DataLoader(padded_trainset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    padded_testloader = torch.utils.data.DataLoader(padded_testset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    # shuffle must be false to maintain sorting by length\n",
    "    \n",
    "    return padded_trainloader, padded_testloader\n",
    "padded_en_trainloader, padded_en_testloader = padded_train_test(en_sentences, en_chartable)\n",
    "padded_jp_trainloader, padded_jp_testloader = padded_train_test(jp_sentences, jp_chartable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jp training\n",
      "loss at epoch 1: 5.0152907371521\n",
      "loss at epoch 2: 1.397065281867981\n",
      "loss at epoch 3: 1.1597495079040527\n",
      "loss at epoch 4: 1.1314160823822021\n",
      "loss at epoch 5: 1.097606897354126\n",
      "loss at epoch 6: 1.0442453622817993\n",
      "loss at epoch 7: 0.9845795631408691\n",
      "loss at epoch 8: 0.9420406222343445\n",
      "loss at epoch 9: 0.9162500500679016\n",
      "loss at epoch 10: 0.8993980288505554\n",
      "loss at epoch 11: 0.8872933387756348\n",
      "loss at epoch 12: 0.8779669404029846\n",
      "loss at epoch 13: 0.8704037666320801\n",
      "loss at epoch 14: 0.8640315532684326\n",
      "loss at epoch 15: 0.8585015535354614\n",
      "loss at epoch 16: 0.8535885810852051\n",
      "loss at epoch 17: 0.8491427302360535\n",
      "loss at epoch 18: 0.8450546264648438\n",
      "loss at epoch 19: 0.8412467837333679\n",
      "loss at epoch 20: 0.8376609683036804\n",
      "loss at epoch 21: 0.8342527747154236\n",
      "loss at epoch 22: 0.8309846520423889\n",
      "loss at epoch 23: 0.8278281092643738\n",
      "loss at epoch 24: 0.8247584700584412\n",
      "loss at epoch 25: 0.8217585682868958\n",
      "loss at epoch 26: 0.8188115954399109\n",
      "loss at epoch 27: 0.8159028887748718\n",
      "loss at epoch 28: 0.8130220174789429\n",
      "loss at epoch 29: 0.8101606965065002\n",
      "loss at epoch 30: 0.8073118329048157\n",
      "loss at epoch 31: 0.8044702410697937\n",
      "loss at epoch 32: 0.8016319274902344\n",
      "loss at epoch 33: 0.7987968921661377\n",
      "loss at epoch 34: 0.7959674596786499\n",
      "loss at epoch 35: 0.7931442260742188\n",
      "loss at epoch 36: 0.7903335094451904\n",
      "loss at epoch 37: 0.7875438332557678\n",
      "loss at epoch 38: 0.7847825884819031\n",
      "loss at epoch 39: 0.7820596098899841\n",
      "loss at epoch 40: 0.7793823480606079\n",
      "loss at epoch 41: 0.7767577767372131\n",
      "loss at epoch 42: 0.7741920948028564\n",
      "loss at epoch 43: 0.7716864943504333\n",
      "loss at epoch 44: 0.7692368030548096\n",
      "loss at epoch 45: 0.7668431997299194\n",
      "loss at epoch 46: 0.7644970417022705\n",
      "loss at epoch 47: 0.7621934413909912\n",
      "loss at epoch 48: 0.7599282264709473\n",
      "loss at epoch 49: 0.7576943039894104\n",
      "loss at epoch 50: 0.755492627620697\n",
      "loss at epoch 51: 0.7533186078071594\n",
      "loss at epoch 52: 0.7511709928512573\n",
      "loss at epoch 53: 0.7490502595901489\n",
      "loss at epoch 54: 0.7469556331634521\n",
      "loss at epoch 55: 0.7448882460594177\n",
      "loss at epoch 56: 0.742850124835968\n",
      "loss at epoch 57: 0.740836501121521\n",
      "loss at epoch 58: 0.7388519048690796\n",
      "loss at epoch 59: 0.7368941903114319\n",
      "loss at epoch 60: 0.7349623441696167\n",
      "loss at epoch 61: 0.7330529093742371\n",
      "loss at epoch 62: 0.7311720848083496\n",
      "loss at epoch 63: 0.7293107509613037\n",
      "loss at epoch 64: 0.7274683713912964\n",
      "loss at epoch 65: 0.7256478071212769\n",
      "loss at epoch 66: 0.7238437533378601\n",
      "loss at epoch 67: 0.7220573425292969\n",
      "loss at epoch 68: 0.7202829122543335\n",
      "loss at epoch 69: 0.7185223698616028\n",
      "loss at epoch 70: 0.7167708873748779\n",
      "loss at epoch 71: 0.7150304317474365\n",
      "loss at epoch 72: 0.71329665184021\n",
      "loss at epoch 73: 0.7115710973739624\n",
      "loss at epoch 74: 0.7098504304885864\n",
      "loss at epoch 75: 0.7081397175788879\n",
      "loss at epoch 76: 0.7064282298088074\n",
      "loss at epoch 77: 0.7047204971313477\n",
      "loss at epoch 78: 0.7030152678489685\n",
      "loss at epoch 79: 0.7013135552406311\n",
      "loss at epoch 80: 0.6996132135391235\n",
      "loss at epoch 81: 0.6979120969772339\n",
      "loss at epoch 82: 0.6962113976478577\n",
      "loss at epoch 83: 0.6945135593414307\n",
      "loss at epoch 84: 0.6928130984306335\n",
      "loss at epoch 85: 0.6911147832870483\n",
      "loss at epoch 86: 0.6894136071205139\n",
      "loss at epoch 87: 0.6877108216285706\n",
      "loss at epoch 88: 0.6860105991363525\n",
      "loss at epoch 89: 0.6843053102493286\n",
      "loss at epoch 90: 0.682604193687439\n",
      "loss at epoch 91: 0.6808978319168091\n",
      "loss at epoch 92: 0.6791935563087463\n",
      "loss at epoch 93: 0.6774848699569702\n",
      "loss at epoch 94: 0.6757799386978149\n",
      "loss at epoch 95: 0.6740692257881165\n",
      "loss at epoch 96: 0.6723613739013672\n",
      "loss at epoch 97: 0.6706483960151672\n",
      "loss at epoch 98: 0.6689398884773254\n",
      "loss at epoch 99: 0.6672282814979553\n",
      "loss at epoch 100: 0.6655137538909912\n",
      "final loss after 100 epochs: 0.6655137538909912\n",
      "en training\n",
      "loss at epoch 1: 2.3041632175445557\n",
      "loss at epoch 2: 0.6920510530471802\n",
      "loss at epoch 3: 0.5925269722938538\n",
      "loss at epoch 4: 0.515401303768158\n",
      "loss at epoch 5: 0.43780091404914856\n",
      "loss at epoch 6: 0.38442447781562805\n",
      "loss at epoch 7: 0.34737613797187805\n",
      "loss at epoch 8: 0.319771945476532\n",
      "loss at epoch 9: 0.29865047335624695\n",
      "loss at epoch 10: 0.2819541096687317\n",
      "loss at epoch 11: 0.2681408226490021\n",
      "loss at epoch 12: 0.2560703754425049\n",
      "loss at epoch 13: 0.2450757771730423\n",
      "loss at epoch 14: 0.23482409119606018\n",
      "loss at epoch 15: 0.2251659333705902\n",
      "loss at epoch 16: 0.21604584157466888\n",
      "loss at epoch 17: 0.2074294537305832\n",
      "loss at epoch 18: 0.19930708408355713\n",
      "loss at epoch 19: 0.1916545033454895\n",
      "loss at epoch 20: 0.18445806205272675\n",
      "loss at epoch 21: 0.1776943951845169\n",
      "loss at epoch 22: 0.17133526504039764\n",
      "loss at epoch 23: 0.16535905003547668\n",
      "loss at epoch 24: 0.15973803400993347\n",
      "loss at epoch 25: 0.15445300936698914\n",
      "loss at epoch 26: 0.14947712421417236\n",
      "loss at epoch 27: 0.14478498697280884\n",
      "loss at epoch 28: 0.14036011695861816\n",
      "loss at epoch 29: 0.13617944717407227\n",
      "loss at epoch 30: 0.13222239911556244\n",
      "loss at epoch 31: 0.12847261130809784\n",
      "loss at epoch 32: 0.12491505593061447\n",
      "loss at epoch 33: 0.1215309202671051\n",
      "loss at epoch 34: 0.11830843240022659\n",
      "loss at epoch 35: 0.11523167788982391\n",
      "loss at epoch 36: 0.11229445040225983\n",
      "loss at epoch 37: 0.10948224365711212\n",
      "loss at epoch 38: 0.10678607225418091\n",
      "loss at epoch 39: 0.1041974127292633\n",
      "loss at epoch 40: 0.10171055048704147\n",
      "loss at epoch 41: 0.09931773692369461\n",
      "loss at epoch 42: 0.0970124751329422\n",
      "loss at epoch 43: 0.09478942304849625\n",
      "loss at epoch 44: 0.09264571964740753\n",
      "loss at epoch 45: 0.09057357907295227\n",
      "loss at epoch 46: 0.08857009559869766\n",
      "loss at epoch 47: 0.08663414418697357\n",
      "loss at epoch 48: 0.08475866168737411\n",
      "loss at epoch 49: 0.0829438865184784\n",
      "loss at epoch 50: 0.08118445426225662\n",
      "loss at epoch 51: 0.07948099821805954\n",
      "loss at epoch 52: 0.07782851904630661\n",
      "loss at epoch 53: 0.0762251541018486\n",
      "loss at epoch 54: 0.07467149943113327\n",
      "loss at epoch 55: 0.07316270470619202\n",
      "loss at epoch 56: 0.07169840484857559\n",
      "loss at epoch 57: 0.0702766478061676\n",
      "loss at epoch 58: 0.06889580935239792\n",
      "loss at epoch 59: 0.06755482405424118\n",
      "loss at epoch 60: 0.06625232100486755\n",
      "loss at epoch 61: 0.06498692184686661\n",
      "loss at epoch 62: 0.06375725567340851\n",
      "loss at epoch 63: 0.06256214529275894\n",
      "loss at epoch 64: 0.06140078231692314\n",
      "loss at epoch 65: 0.06027085706591606\n",
      "loss at epoch 66: 0.059172749519348145\n",
      "loss at epoch 67: 0.05810467526316643\n",
      "loss at epoch 68: 0.05706486850976944\n",
      "loss at epoch 69: 0.056054264307022095\n",
      "loss at epoch 70: 0.05507100373506546\n",
      "loss at epoch 71: 0.054113756865262985\n",
      "loss at epoch 72: 0.05318156257271767\n",
      "loss at epoch 73: 0.05227448791265488\n",
      "loss at epoch 74: 0.05139097198843956\n",
      "loss at epoch 75: 0.05053135007619858\n",
      "loss at epoch 76: 0.04969340190291405\n",
      "loss at epoch 77: 0.048876769840717316\n",
      "loss at epoch 78: 0.048081956803798676\n",
      "loss at epoch 79: 0.04730692505836487\n",
      "loss at epoch 80: 0.04655176028609276\n",
      "loss at epoch 81: 0.045815128833055496\n",
      "loss at epoch 82: 0.045097094029188156\n",
      "loss at epoch 83: 0.044396836310625076\n",
      "loss at epoch 84: 0.043714459985494614\n",
      "loss at epoch 85: 0.043048419058322906\n",
      "loss at epoch 86: 0.042398758232593536\n",
      "loss at epoch 87: 0.04176457226276398\n",
      "loss at epoch 88: 0.04114557430148125\n",
      "loss at epoch 89: 0.04054123908281326\n",
      "loss at epoch 90: 0.039951179176568985\n",
      "loss at epoch 91: 0.03937484696507454\n",
      "loss at epoch 92: 0.038812097162008286\n",
      "loss at epoch 93: 0.03826257959008217\n",
      "loss at epoch 94: 0.037725869566202164\n",
      "loss at epoch 95: 0.03720061480998993\n",
      "loss at epoch 96: 0.03668783977627754\n",
      "loss at epoch 97: 0.03618599846959114\n",
      "loss at epoch 98: 0.03569469228386879\n",
      "loss at epoch 99: 0.03521527722477913\n",
      "loss at epoch 100: 0.03474585339426994\n",
      "final loss after 100 epochs: 0.03474585339426994\n"
     ]
    }
   ],
   "source": [
    "def train_char(jp=True, lr=.1, epochs=100):\n",
    "    trainloader = padded_en_trainloader\n",
    "    chartable = en_chartable\n",
    "    if jp:\n",
    "        trainloader = padded_jp_trainloader\n",
    "        chartable = jp_chartable\n",
    "    model = CharacterPredictor(chartable).to(device)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.NLLLoss()\n",
    "    losses = []\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        num_losses = 0\n",
    "        for index, data in enumerate(trainloader, 0):\n",
    "            model.zero_grad()\n",
    "            padded_seq, lengths = data\n",
    "            pred = model(padded_seq, lengths)\n",
    "            \n",
    "            batch_size = padded_seq.shape[0]\n",
    "            maxlen = padded_seq.shape[1]\n",
    "            vocab_size = pred.shape[-1]\n",
    "            padded_seq_flat = padded_seq.view(batch_size*maxlen)\n",
    "            pred_flat = pred.contiguous().view(batch_size*maxlen, vocab_size)\n",
    "            \n",
    "            loss = loss_fn(pred_flat, padded_seq_flat)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.data\n",
    "            num_losses += 1\n",
    "        avg_loss = total_loss / num_losses\n",
    "        losses.append(avg_loss)\n",
    "        print('loss at epoch {}: {}'.format(epoch+1, avg_loss))\n",
    "    print('final loss after {} epochs: {}'.format(epochs, losses[-1]))\n",
    "    return model, losses\n",
    "print('jp training')\n",
    "jp_model, jp_losses = train_char()\n",
    "print('en training')\n",
    "en_model, en_losses = train_char(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics\n",
    "def print_metrics(model, name, testloader):\n",
    "    loss_fn = nn.NLLLoss()\n",
    "    losses = []\n",
    "    sentence_accuracies = []\n",
    "    character_accuracies = []\n",
    "    for index, data in enumerate(testloader, 0):\n",
    "        padded_seq, lengths = data\n",
    "        pred, maxInds = model.predict(padded_seq, lengths)\n",
    "        batch_size = padded_seq.shape[0]\n",
    "        maxlen = padded_seq.shape[1]\n",
    "        vocab_size = pred.shape[-1]\n",
    "        padded_seq_flat = padded_seq.view(batch_size*maxlen)\n",
    "        pred_flat = pred.contiguous().view(batch_size*maxlen, vocab_size)\n",
    "        loss = loss_fn(pred_flat, padded_seq_flat).data\n",
    "        correct_characters = torch.sum(max_ind == padded_seq)\n",
    "        total_characters = batch_size*maxlen\n",
    "        correct_sentences = 0\n",
    "        total_sentences = batch_size\n",
    "        for i in range(batch_size):\n",
    "            if torch.all(maxInds[i] == padded_seq[i]):\n",
    "                correct_sentences += 1\n",
    "        sentence_accuracy = correct_sentences / total_sentences\n",
    "        character_accuracy = correct_characters / total_characters\n",
    "        \n",
    "        losses.append(loss)\n",
    "        sentence_accuracies.append(sentence_accuracy)\n",
    "        character_accuracies.append(character_accuracy)\n",
    "    loss_avg = sum(losses) / len(losses)\n",
    "    sentence_accuracy_avg = sum(sentence_accuracies) / len(sentence_accuracies)\n",
    "    character_accuracy_avg = sum(character_accuracies) / len(character_accuracies)\n",
    "    print('model: {}, validation loss: {}, sentence accuracy: {}, character accuracy: {}'.format(name, loss_avg, sentence_accuracy_avg, character_accuracy_avg))\n",
    "print_metrics(jp_model, 'jp character predictor', padded_jp_testloader)\n",
    "print_metrics(en_model, 'en character predictor', padded_en_testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1 + 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
