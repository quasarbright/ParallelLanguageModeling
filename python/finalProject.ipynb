{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from IPython.core.debugger import set_trace\n",
    "# set device\n",
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['102世吉田日厚貫首', '1月15日：成人祭、新年祭', '1月3日：家運隆盛、商売繁盛祈願祭', '1月7日：七種粥神事', '21世紀COEプログラム']\n",
      "['the 102nd head priest, Nikko TOSHIDA', '15th January: Seijin-sai (Adult Festival), the New Year Festival', '3rd January: Prayer Festival for the prosperity of family fortunes and business', '7th January: Nanakusa-gayu shinji (a divine service for a rice porridge with seven spring herbs to insure health for the new year)', 'The 21st Century Center Of Excellence Program']\n",
      "51982\n",
      "51982\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "jp_sentences = []\n",
    "en_sentences = []\n",
    "with open('data/kyoto_lexicon.csv', 'r', encoding='utf-8') as file:\n",
    "    reader = csv.reader(file, delimiter=',')\n",
    "    # skip the header row\n",
    "    startLooking = False\n",
    "    for row in reader:\n",
    "        if startLooking:\n",
    "            jp_sentences.append(row[0])\n",
    "            en_sentences.append(row[1])\n",
    "        startLooking = True\n",
    "print(jp_sentences[:5])\n",
    "print(en_sentences[:5])\n",
    "print(len(jp_sentences))\n",
    "print(len(en_sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# character-by-character prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[148, 169], [9, 45]]\n",
      "信\n",
      "3911 173\n"
     ]
    }
   ],
   "source": [
    "# encoding and decoding characters\n",
    "class CharacterTable:\n",
    "    def __init__(self, charset):\n",
    "        self.charset = charset\n",
    "        self.charset = frozenset(self.charset)\n",
    "        self.charlist = ['<null>', '<sos>'] + list(self.charset)\n",
    "        self.weights = torch.ones((len(self.charlist),)).to(device)\n",
    "        self.weights[0] = 0\n",
    "        # it is important that null is at index 0 since padding fills with zeroes\n",
    "        self.vocab_size = len(self.charlist)\n",
    "    def encode(self, char):\n",
    "        '''convert from character to index\n",
    "        can process (nested) list of characters'''\n",
    "        if type(char) is type('asdf'):\n",
    "            # char is a string\n",
    "            return self.charlist.index(char)\n",
    "        else:\n",
    "            # char is a list of strings\n",
    "            return [self.encode(char) for char in char]\n",
    "    def decode(self, charInd):\n",
    "        '''convert from index to character\n",
    "        can process (nested) list of indices'''\n",
    "        if type(charInd) is type(22):\n",
    "            # charInd is an int\n",
    "            return self.charlist[charInd]\n",
    "        else:\n",
    "            # charInd is a list of ints\n",
    "            return [self.decode(charInd) for charInd in charInd]\n",
    "jp_chartable = CharacterTable(set(''.join(jp_sentences)))\n",
    "en_chartable = CharacterTable(set(''.join(en_sentences)))\n",
    "print(en_chartable.encode([['a', 'b'], ['c', 'd']]))\n",
    "print(jp_chartable.decode(1234))\n",
    "print(jp_chartable.vocab_size, en_chartable.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequence prediction model\n",
    "class Predictor(nn.Module):\n",
    "    def __init__(self, table, embedding_dimensions=64, hidden_size=100):\n",
    "        super(Predictor, self).__init__()\n",
    "        # model constants\n",
    "        self.embedding_dimensions = embedding_dimensions\n",
    "        self.hidden_size = hidden_size\n",
    "        self.table = table\n",
    "        self.vocab_size = self.table.vocab_size\n",
    "        # model layers\n",
    "        self.embedding = nn.Embedding(self.vocab_size, embedding_dimensions)\n",
    "        self.RNN = nn.LSTM(\n",
    "            input_size=self.embedding_dimensions,\n",
    "            hidden_size=self.hidden_size, \n",
    "            batch_first=True\n",
    "        )\n",
    "        # linear layer for converting from hidden state to softmax\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(self.hidden_size, self.vocab_size),\n",
    "            nn.LogSoftmax(dim=-1)\n",
    "        )\n",
    "    \n",
    "    \n",
    "    def forward(self, padded_seq, lengths):\n",
    "        '''\n",
    "        predicts sequence of characters at every step\n",
    "        seq (batch, seq) padded tensor of character indices\n",
    "        returns (batch, seq, vocab) softmaxes\n",
    "        implicit teacher forcing by torch RNN\n",
    "        '''\n",
    "        seq_len = padded_seq.shape[1]\n",
    "        padded_seq_embed = self.embedding(padded_seq) # (batch, seq, embed)\n",
    "        packed_seq_embed = torch.nn.utils.rnn.pack_padded_sequence(padded_seq_embed, lengths, batch_first=True)\n",
    "        packed_hidden_states, (h_final, cell_final) = self.RNN(packed_seq_embed)\n",
    "        padded_hidden_states, input_sizes = torch.nn.utils.rnn.pad_packed_sequence(packed_hidden_states, batch_first=True, total_length=seq_len)\n",
    "        # hidden_states (batch, seq, hidden) hidden states\n",
    "        y_hat = self.linear(padded_hidden_states)\n",
    "        # y_hat (batch, seq, vocab) softmaxes\n",
    "        return y_hat\n",
    "    \n",
    "    \n",
    "    def predict(self, padded_seq, lengths):\n",
    "        pred = self.forward(padded_seq, lengths)\n",
    "        # (batch, seq, vocab)\n",
    "        maxInds = pred.max(2)[1]\n",
    "        # (batch, seq)\n",
    "        return pred, maxInds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "def padded_train_test(sentences, table, train_test_split=.2, batch_size=100, word=False):\n",
    "    '''\n",
    "    small train_test_split means mostly train data\n",
    "    ['hello world', ...] or [['hello', 'world',...],...], table, train_test_split -> (train data, test data) padded tensor dataloaders\n",
    "    small train_test_split means mostly train data\n",
    "    output \"shapes\" (train_size, maxlen), (test_size, maxlen) with given batch size\n",
    "    '''\n",
    "    \n",
    "    def encode_sequence(sentences, ans):\n",
    "        # ans is whether this is the input sequence or the true sequence\n",
    "        # ans=True means don't append an <sos>\n",
    "        # ans=False means append an <sos> and remove the last\n",
    "        sentence_indices = []\n",
    "        for sentence in sentences:\n",
    "            if word:\n",
    "                encoded = table.encode(sentence)\n",
    "            else:\n",
    "                encoded = table.encode(list(sentence))\n",
    "            if not ans:\n",
    "                # add sos and remove last\n",
    "                # this is an input sequence\n",
    "                encoded = [table.encode('<sos>')] + encoded[:-1]\n",
    "            sentence_indices.append(encoded)\n",
    "        return sentence_indices\n",
    "    def pad_sequence(sentences, ans):\n",
    "        # ans is whether this is the input sequence or the true sequence\n",
    "        # ans=True means don't append an <sos>\n",
    "        # ans=False means append an <sos> and remove the last\n",
    "        '''\n",
    "        ['hello world', ...] or [['hello', 'world',...],...] -> (padded long tensor, lengths tensor)\n",
    "        tensors are padded and sorted \n",
    "        '''\n",
    "        sentence_indices = encode_sequence(sentences, ans)\n",
    "        # list of list of indices\n",
    "        lengths = torch.LongTensor([len(sentence) for sentence in sentence_indices])\n",
    "        sentence_tensors = [torch.LongTensor(sentence).to(device) for sentence in  sentence_indices]\n",
    "        padded = torch.nn.utils.rnn.pad_sequence(sentence_tensors, batch_first=True)\n",
    "        lengths, perm_idx = lengths.sort(0, descending=True)\n",
    "        # perm_idx is the permutation of sentence indices as sorted by length\n",
    "        padded = padded[perm_idx]\n",
    "        return padded, lengths\n",
    "    \n",
    "    length = len(sentences)\n",
    "    # the index to separate train from test\n",
    "    split = int(length * train_test_split)\n",
    "    \n",
    "    # shuffle before splitting so test doesn't just get the alphabetically sooner sentences\n",
    "    sentences = random.sample(sentences, length)\n",
    "    \n",
    "    train_sentences = sentences[split:]\n",
    "    test_sentences = sentences[:split]\n",
    "    \n",
    "    # the input sequences (with sos and removed last)\n",
    "    padded_train_in = pad_sequence(train_sentences, False)\n",
    "    padded_test_in = pad_sequence(test_sentences, False)\n",
    "    # the output sequences (with no sos)\n",
    "    padded_train_true = pad_sequence(train_sentences, True)\n",
    "    padded_test_true = pad_sequence(test_sentences, True)\n",
    "    \n",
    "    padded_trainset = torch.utils.data.TensorDataset(*padded_train_in, *padded_train_true)\n",
    "    padded_testset = torch.utils.data.TensorDataset(*padded_test_in, *padded_test_true)\n",
    "    \n",
    "    padded_trainloader = torch.utils.data.DataLoader(padded_trainset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    padded_testloader = torch.utils.data.DataLoader(padded_testset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    # shuffle must be false to maintain sorting by length\n",
    "    \n",
    "    return padded_trainloader, padded_testloader\n",
    "padded_en_trainloader, padded_en_testloader = padded_train_test(en_sentences, en_chartable)\n",
    "padded_jp_trainloader, padded_jp_testloader = padded_train_test(jp_sentences, jp_chartable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(trainloader, table, lr=.1, epochs=400):\n",
    "    model = Predictor(table).to(device)\n",
    "    model.train()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.NLLLoss(weight=table.weights)\n",
    "    losses = []\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        num_losses = 0\n",
    "        for index, data in enumerate(trainloader, 0):\n",
    "            model.zero_grad()\n",
    "            padded_seq_in, lengths_in, padded_seq_true, lengths_true = data\n",
    "            pred = model(padded_seq_in, lengths_in)\n",
    "            \n",
    "            batch_size = padded_seq_true.shape[0]\n",
    "            maxlen = padded_seq_true.shape[1]\n",
    "            vocab_size = pred.shape[-1]\n",
    "            padded_seq_true_flat = padded_seq_true.view(batch_size*maxlen)\n",
    "            pred_flat = pred.contiguous().view(batch_size*maxlen, vocab_size)\n",
    "            \n",
    "            loss = loss_fn(pred_flat, padded_seq_true_flat)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.data\n",
    "            num_losses += 1\n",
    "        avg_loss = total_loss / num_losses\n",
    "        losses.append(avg_loss)\n",
    "        if (epoch + 1) % (epochs // 10) == 0:\n",
    "            print('loss at epoch {}: {}'.format(epoch+1, avg_loss))\n",
    "    print('final loss after {} epochs: {}'.format(epochs, losses[-1]))\n",
    "    return model, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save and load model\n",
    "def get_state_path(name):\n",
    "    return 'states/{}.pt'.format(name)\n",
    "def save_model(model, name):\n",
    "    torch.save(model, get_state_path(name))\n",
    "def load_model(model, name):\n",
    "    '''loads state dict into given model and returns it'''\n",
    "    model = torch.load(get_state_path(name))\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jp training\n",
      "loss at epoch 40: 6.17901086807251\n",
      "loss at epoch 80: 5.814774513244629\n",
      "loss at epoch 120: 5.586328506469727\n",
      "loss at epoch 160: 5.421003341674805\n",
      "loss at epoch 200: 5.2902512550354\n",
      "loss at epoch 240: 5.183859825134277\n",
      "loss at epoch 280: 5.093771934509277\n",
      "loss at epoch 320: 5.016167640686035\n",
      "loss at epoch 360: 4.940596580505371\n",
      "loss at epoch 400: 4.870871067047119\n",
      "final loss after 400 epochs: 4.870871067047119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\mthun\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\torch\\serialization.py:251: UserWarning: Couldn't retrieve source code for container of type Predictor. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en training\n",
      "loss at epoch 40: 2.009737730026245\n",
      "loss at epoch 80: 1.8973212242126465\n",
      "loss at epoch 120: 1.8353677988052368\n",
      "loss at epoch 160: 1.7929754257202148\n",
      "loss at epoch 200: 1.760874629020691\n",
      "loss at epoch 240: 1.735688328742981\n",
      "loss at epoch 280: 1.715451717376709\n",
      "loss at epoch 320: 1.6983892917633057\n",
      "loss at epoch 360: 1.683674931526184\n",
      "loss at epoch 400: 1.6709299087524414\n",
      "final loss after 400 epochs: 1.6709299087524414\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Predictor(\n",
       "   (embedding): Embedding(3911, 64)\n",
       "   (RNN): LSTM(64, 100, batch_first=True)\n",
       "   (linear): Sequential(\n",
       "     (0): Linear(in_features=100, out_features=3911, bias=True)\n",
       "     (1): LogSoftmax()\n",
       "   )\n",
       " ), Predictor(\n",
       "   (embedding): Embedding(173, 64)\n",
       "   (RNN): LSTM(64, 100, batch_first=True)\n",
       "   (linear): Sequential(\n",
       "     (0): Linear(in_features=100, out_features=173, bias=True)\n",
       "     (1): LogSoftmax()\n",
       "   )\n",
       " ))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def initialize_models(should_train=True):\n",
    "    global jp_model, en_model\n",
    "    jp_model = Predictor(jp_chartable).to(device)\n",
    "    jp_losses = None\n",
    "    en_model = Predictor(en_chartable).to(device)\n",
    "    en_losses = None\n",
    "    if should_train:\n",
    "        print('jp training')\n",
    "        jp_model, jp_losses = train_model(padded_jp_trainloader, jp_chartable)\n",
    "        save_model(jp_model, 'jp_char_model')\n",
    "        print('en training')\n",
    "        en_model, en_losses = train_model(padded_en_trainloader, en_chartable)\n",
    "        save_model(en_model, 'en_char_model')\n",
    "    else:\n",
    "        jp_model = load_model(jp_model, 'jp_char_model')\n",
    "        en_model = load_model(en_model, 'en_char_model') \n",
    "    return jp_model, en_model\n",
    "initialize_models(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# english word-to-word\n",
    "since the japanese model had to learn a mixture of character prediction and word prediction at the same time, let's see how the english model predicts words, and compare it to the japanese character predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mthun\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', '102nd', 'head', 'priest', ',', 'Nikko', 'TOSHIDA']\n"
     ]
    }
   ],
   "source": [
    "# tokenize sentences\n",
    "tokenized_sentences = []\n",
    "for sentence in en_sentences:\n",
    "    tokenized = nltk.word_tokenize(sentence)\n",
    "    if len(tokenized) > 0:\n",
    "        tokenized_sentences.append(tokenized)\n",
    "print(tokenized_sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43216"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordlist = []\n",
    "for sentence in tokenized_sentences:\n",
    "    for word in sentence:\n",
    "        wordlist.append(word)\n",
    "wordset = set(wordlist)\n",
    "len(wordset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### that's way too many words!\n",
    "let's limit the vocab size to 4000 to make the complexity theoretically similar to the japanese model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "of\t6995\n",
      "(\t6793\n",
      ")\t6769\n",
      "the\t5777\n",
      ",\t3457\n",
      "no\t2899\n",
      "a\t2872\n",
      "Temple\t1617\n",
      "and\t1278\n",
      "in\t1175\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_vocab_size = 20000\n",
    "# word -> frequency\n",
    "counts = {}\n",
    "for word in wordlist:\n",
    "    if word in counts:\n",
    "        counts[word] += 1\n",
    "    else:\n",
    "        counts[word] = 1\n",
    "sorted_wordset = sorted(list(wordset), key=lambda word: counts[word], reverse=True)\n",
    "for word in sorted_wordset[:10]:\n",
    "    print(word, counts[word], sep='\\t')\n",
    "vocab = set([])\n",
    "for word in sorted_wordset:\n",
    "    if len(vocab) < max_vocab_size:\n",
    "        vocab.add(word)\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shimogyo\n",
      "<unk>\n",
      "20003\n"
     ]
    }
   ],
   "source": [
    "# word encoding and decoding\n",
    "class WordTable:\n",
    "    def __init__(self, wordset):\n",
    "        self.wordset = frozenset(wordset)\n",
    "        self.wordlist = ['<null>', '<sos>', '<unk>'] + list(wordset)\n",
    "        self.weights = torch.ones((len(self.wordlist),)).to(device)\n",
    "        self.weights[0] = 0\n",
    "        self.weights[2] = 0\n",
    "        self.vocab_size = len(self.wordlist)\n",
    "        \n",
    "        \n",
    "    def encode(self, word):\n",
    "        '''\n",
    "        expects word string or possibly nested list of word strings\n",
    "        unks out-of-vocab words\n",
    "        word(s) -> indices\n",
    "        '''\n",
    "        if type(word) == type('asdf'):\n",
    "            if word in self.wordlist:\n",
    "                return self.wordlist.index(word)\n",
    "            else:\n",
    "                # encode out-of-vocab words with unk\n",
    "                return self.wordlist.index('<unk>')\n",
    "        else:\n",
    "            words = word\n",
    "            return [self.encode(word) for word in words]\n",
    "        \n",
    "        \n",
    "    def decode(self, wordInd):\n",
    "        '''\n",
    "        expects wordInd index or possibly nested list of word indices\n",
    "        '''\n",
    "        if type(wordInd) == type(123):\n",
    "            return self.wordlist[wordInd]\n",
    "        else:\n",
    "            wordInds = wordInd\n",
    "            return [self.decode(wordInd) for wordInd in wordInds]\n",
    "wordtable = WordTable(vocab)\n",
    "print(wordtable.decode(200))\n",
    "print(wordtable.decode(wordtable.encode('why relu works')))\n",
    "print(wordtable.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "padded_word_trainloader, padded_word_testloader = padded_train_test(tokenized_sentences, wordtable, word=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training english word model\n",
      "loss at epoch 40: 7.60959529876709\n",
      "loss at epoch 80: 7.372646331787109\n",
      "loss at epoch 120: 7.229129314422607\n",
      "loss at epoch 160: 7.08121919631958\n",
      "loss at epoch 200: 6.958672523498535\n",
      "loss at epoch 240: 6.848971843719482\n",
      "loss at epoch 280: 6.751115798950195\n",
      "loss at epoch 320: 6.647672653198242\n",
      "loss at epoch 360: 6.5640482902526855\n",
      "loss at epoch 400: 6.477436065673828\n",
      "final loss after 400 epochs: 6.477436065673828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\mthun\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\torch\\serialization.py:251: UserWarning: Couldn't retrieve source code for container of type Predictor. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "print('training english word model')\n",
    "word_model, word_model_losses = train_model(padded_word_trainloader, wordtable)\n",
    "save_model(word_model, 'word_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics\n",
    "def perplexity_metric(pred, actual, masked_inds):\n",
    "    '''\n",
    "    pred (batch, seq, vocab) logsoftmax\n",
    "    actual (batch, seq) longs\n",
    "    geometric mean of product of p(next word | previous words) for whole sentence\n",
    "    average (arithmetic mean) by batch\n",
    "    '''\n",
    "    batch_size, seq_len, vocab_size = pred.shape\n",
    "    pred = pred.cpu()\n",
    "    actual = actual.cpu()\n",
    "    pred = torch.exp(pred)\n",
    "    geo_means = [] # probabilities of correct characters\n",
    "    for i in range(batch_size):\n",
    "        product = 1\n",
    "        num_factors = 0\n",
    "        curr_pred = pred[i]\n",
    "        curr_actual = actual[i]\n",
    "        for t in range(seq_len):\n",
    "            trueInd = curr_actual[t].item()\n",
    "            # the character index at this timestep\n",
    "            if trueInd not in masked_inds:\n",
    "                # we don't care how well it predicts nulls or unks\n",
    "                predSoftmax = curr_pred[t]\n",
    "                confidence = predSoftmax[trueInd].item()\n",
    "                product *= confidence\n",
    "                num_factors += 1\n",
    "        if num_factors != 0:\n",
    "            geo_means.append(product ** (1/num_factors))\n",
    "    return sum(geo_means) / len(geo_means)\n",
    "def print_metrics(model, name, testloader, word=False):\n",
    "    class_weights = model.table.weights\n",
    "    masked_inds = []\n",
    "    for index, weight in enumerate(class_weights):\n",
    "        if weight.item() == 0:\n",
    "            masked_inds.append(index)\n",
    "    loss_fn = nn.NLLLoss(weight=model.table.weights)\n",
    "    losses = []\n",
    "    sentence_accuracies = []\n",
    "    character_accuracies = []\n",
    "    perplexities = []\n",
    "    for index, data in enumerate(testloader, 0):\n",
    "        padded_seq_in, lengths_in, padded_seq, lengths = data\n",
    "        \n",
    "        batch_size = padded_seq.shape[0]\n",
    "        maxlen = padded_seq.shape[1]\n",
    "        \n",
    "        mask = torch.ones(padded_seq.shape).to(device)\n",
    "        for i in range(batch_size):\n",
    "            for t in range(maxlen):\n",
    "                value = padded_seq[i,t]\n",
    "                if value in masked_inds:\n",
    "                    mask[i,t] = 0\n",
    "        num_masked = batch_size*maxlen - torch.sum(mask).item() # how many zeros in the mask?\n",
    "        \n",
    "        pred, maxInds = model.predict(padded_seq, lengths)\n",
    "        vocab_size = pred.shape[-1]\n",
    "        \n",
    "        perplexity = perplexity_metric(pred, padded_seq, masked_inds)\n",
    "        \n",
    "        padded_seq_flat = padded_seq.view(batch_size*maxlen)\n",
    "        pred_flat = pred.contiguous().view(batch_size*maxlen, vocab_size)\n",
    "        loss = loss_fn(pred_flat, padded_seq_flat).item()\n",
    "        \n",
    "        maxInds = maxInds.float()*mask\n",
    "        padded_seq = padded_seq.float()*mask\n",
    "        \n",
    "        correct_characters = torch.sum(maxInds == padded_seq).item() - num_masked\n",
    "        total_characters = batch_size*maxlen - num_masked # exclude masked indices from prediction\n",
    "        # we don't need to worry about unks, nulls, and masking in sentence because\n",
    "        # it'll only check if the \"good\" parts of the sentence are the same\n",
    "        correct_sentences = 0\n",
    "        total_sentences = batch_size\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            if torch.all(maxInds[i] == padded_seq[i]):\n",
    "                correct_sentences += 1\n",
    "        sentence_accuracy = correct_sentences / total_sentences\n",
    "        character_accuracy = correct_characters / total_characters\n",
    "        \n",
    "        losses.append(loss)\n",
    "        sentence_accuracies.append(sentence_accuracy)\n",
    "        character_accuracies.append(character_accuracy)\n",
    "        perplexities.append(perplexity)\n",
    "    loss_avg = sum(losses) / len(losses)\n",
    "    sentence_accuracy_avg = sum(sentence_accuracies) / len(sentence_accuracies)\n",
    "    character_accuracy_avg = sum(character_accuracies) / len(character_accuracies)\n",
    "    perplexity_avg = sum(perplexities) / len(perplexities)\n",
    "    if word:\n",
    "        print('model: {}\\n\\tvalidation loss: {}\\n\\tsentence accuracy: {}\\n\\tword accuracy: {}\\n\\tperplexity: {}'.format(name, loss_avg, sentence_accuracy_avg, character_accuracy_avg, perplexity_avg))\n",
    "    else:\n",
    "        print('model: {}\\n\\tvalidation loss: {}\\n\\tsentence accuracy: {}\\n\\tcharacter accuracy: {}\\n\\tperplexity: {}'.format(name, loss_avg, sentence_accuracy_avg, character_accuracy_avg, perplexity_avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: jp character predictor\n",
      "\tvalidation loss: 7.284843334784875\n",
      "\tsentence accuracy: 0.0\n",
      "\tcharacter accuracy: 0.004010157009820773\n",
      "\tperplexity: 0.0009799587880267764\n",
      "model: en character predictor\n",
      "\tvalidation loss: 7.215273389449487\n",
      "\tsentence accuracy: 0.0\n",
      "\tcharacter accuracy: 0.015122865669507742\n",
      "\tperplexity: 0.0011429361315950432\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-4c7e1e4a1677>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprint_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjp_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'jp character predictor'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadded_jp_testloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mprint_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0men_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'en character predictor'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadded_en_testloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mprint_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'english word to word'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadded_word_testloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-24-ca9cb5762f96>\u001b[0m in \u001b[0;36mprint_metrics\u001b[1;34m(model, name, testloader, word)\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[0mvocab_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m         \u001b[0mperplexity\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mperplexity_metric\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadded_seq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmasked_inds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[0mpadded_seq_flat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpadded_seq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mmaxlen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-24-ca9cb5762f96>\u001b[0m in \u001b[0;36mperplexity_metric\u001b[1;34m(pred, actual, masked_inds)\u001b[0m\n\u001b[0;32m     26\u001b[0m                 \u001b[0mproduct\u001b[0m \u001b[1;33m*=\u001b[0m \u001b[0mconfidence\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m                 \u001b[0mnum_factors\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[0mgeo_means\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproduct\u001b[0m \u001b[1;33m**\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mnum_factors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgeo_means\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgeo_means\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mprint_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "print_metrics(jp_model, 'jp character predictor', padded_jp_testloader)\n",
    "print_metrics(en_model, 'en character predictor', padded_en_testloader)\n",
    "print_metrics(word_model, 'english word to word', padded_word_testloader, word=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentence(model, table, loader):\n",
    "    seq_in, len_in, seq_true, len_true = iter(loader).next()\n",
    "    pred, maxInds = model.predict(seq_in, len_in)\n",
    "    trues = table.decode(seq_true[200:201].cpu().numpy().tolist())\n",
    "    preds = table.decode(maxInds[200:201].cpu().detach().numpy().tolist())\n",
    "    zipped = list(zip(trues, preds))\n",
    "    for t, p in zipped:\n",
    "        print(t)\n",
    "        print(p)\n",
    "predict_sentence(word_model, wordtable, padded_word_testloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
