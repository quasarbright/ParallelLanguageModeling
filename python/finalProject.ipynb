{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from IPython.core.debugger import set_trace\n",
    "# set device\n",
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['102世吉田日厚貫首', '1月15日：成人祭、新年祭', '1月3日：家運隆盛、商売繁盛祈願祭', '1月7日：七種粥神事', '21世紀COEプログラム']\n",
      "['the 102nd head priest, Nikko TOSHIDA', '15th January: Seijin-sai (Adult Festival), the New Year Festival', '3rd January: Prayer Festival for the prosperity of family fortunes and business', '7th January: Nanakusa-gayu shinji (a divine service for a rice porridge with seven spring herbs to insure health for the new year)', 'The 21st Century Center Of Excellence Program']\n",
      "51982\n",
      "51982\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "jp_sentences = []\n",
    "en_sentences = []\n",
    "with open('data/kyoto_lexicon.csv', 'r', encoding='utf-8') as file:\n",
    "    reader = csv.reader(file, delimiter=',')\n",
    "    # skip the header row\n",
    "    startLooking = False\n",
    "    for row in reader:\n",
    "        if startLooking:\n",
    "            jp_sentences.append(row[0])\n",
    "            en_sentences.append(row[1])\n",
    "        startLooking = True\n",
    "print(jp_sentences[:5])\n",
    "print(en_sentences[:5])\n",
    "print(len(jp_sentences))\n",
    "print(len(en_sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# character-by-character prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[161, 152], [38, 75]]\n",
      "彦\n",
      "3910 172\n"
     ]
    }
   ],
   "source": [
    "# encoding and decoding characters\n",
    "class CharacterTable:\n",
    "    def __init__(self, charset):\n",
    "        self.charset = charset\n",
    "        self.charset = frozenset(self.charset)\n",
    "        self.charlist = ['<null>'] + list(self.charset)\n",
    "        # it is important that null is at index 0 since padding fills with zeroes\n",
    "        self.vocab_size = len(self.charlist)\n",
    "    def encode(self, char):\n",
    "        '''convert from character to index\n",
    "        can process (nested) list of characters'''\n",
    "        if type(char) is type('asdf'):\n",
    "            # char is a string\n",
    "            return self.charlist.index(char)\n",
    "        else:\n",
    "            # char is a list of strings\n",
    "            return [self.encode(char) for char in char]\n",
    "    def decode(self, charInd):\n",
    "        '''convert from index to character\n",
    "        can process (nested) list of indices'''\n",
    "        if type(charInd) is type(22):\n",
    "            # charInd is an int\n",
    "            return self.charlist[charInd]\n",
    "        else:\n",
    "            # charInd is a list of ints\n",
    "            return [self.encode(charInd) for charInd in charInd]\n",
    "jp_chartable = CharacterTable(set(''.join(jp_sentences)))\n",
    "en_chartable = CharacterTable(set(''.join(en_sentences)))\n",
    "print(en_chartable.encode([['a', 'b'], ['c', 'd']]))\n",
    "print(jp_chartable.decode(1234))\n",
    "print(jp_chartable.vocab_size, en_chartable.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequence prediction model\n",
    "class Predictor(nn.Module):\n",
    "    def __init__(self, table, embedding_dimensions=64, hidden_size=100):\n",
    "        super(Predictor, self).__init__()\n",
    "        # model constants\n",
    "        self.embedding_dimensions = embedding_dimensions\n",
    "        self.hidden_size = hidden_size\n",
    "        self.table = table\n",
    "        self.vocab_size = self.table.vocab_size\n",
    "        # model layers\n",
    "        self.embedding = nn.Embedding(self.vocab_size, embedding_dimensions)\n",
    "        self.RNN = nn.LSTM(\n",
    "            input_size=self.embedding_dimensions,\n",
    "            hidden_size=self.hidden_size, \n",
    "            batch_first=True\n",
    "        )\n",
    "        # linear layer for converting from hidden state to softmax\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(self.hidden_size, self.vocab_size),\n",
    "            nn.LogSoftmax(dim=-1)\n",
    "        )\n",
    "    \n",
    "    \n",
    "    def forward(self, padded_seq, lengths):\n",
    "        '''\n",
    "        predicts sequence of characters at every step\n",
    "        seq (batch, seq) padded tensor of character indices\n",
    "        returns (batch, seq, vocab) softmaxes\n",
    "        implicit teacher forcing by torch RNN\n",
    "        '''\n",
    "        seq_len = padded_seq.shape[1]\n",
    "        padded_seq_embed = self.embedding(padded_seq) # (batch, seq, embed)\n",
    "        packed_seq_embed = torch.nn.utils.rnn.pack_padded_sequence(padded_seq_embed, lengths, batch_first=True)\n",
    "        packed_hidden_states, (h_final, cell_final) = self.RNN(packed_seq_embed)\n",
    "        padded_hidden_states, input_sizes = torch.nn.utils.rnn.pad_packed_sequence(packed_hidden_states, batch_first=True, total_length=seq_len)\n",
    "        # hidden_states (batch, seq, hidden) hidden states\n",
    "        y_hat = self.linear(padded_hidden_states)\n",
    "        # y_hat (batch, seq, vocab) softmaxes\n",
    "        return y_hat\n",
    "    \n",
    "    \n",
    "    def predict(self, padded_seq, lengths):\n",
    "        pred = self.forward(padded_seq, lengths)\n",
    "        # (batch, seq, vocab)\n",
    "        maxInds = pred.max(2)[1]\n",
    "        # (batch, seq)\n",
    "        return pred, maxInds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "def padded_train_test(sentences, table, train_test_split=.2, batch_size=500, word=False):\n",
    "    '''\n",
    "    small train_test_split means mostly train data\n",
    "    ['hello world', ...] or [['hello', 'world',...],...], table, train_test_split -> (train data, test data) padded tensor dataloaders\n",
    "    small train_test_split means mostly train data\n",
    "    output \"shapes\" (train_size, maxlen), (test_size, maxlen) with given batch size\n",
    "    '''\n",
    "    def pad_sequence(sentences):\n",
    "        '''\n",
    "        ['hello world', ...] or [['hello', 'world',...],...] -> (padded long tensor, lengths tensor)\n",
    "        tensors are padded and sorted \n",
    "        '''\n",
    "        sentence_indices = [table.encode(list(sentence)) for sentence in sentences]\n",
    "        if word:\n",
    "            sentence_indices = [table.encode(sentence) for sentence in sentences]\n",
    "        # list of list of indices\n",
    "        lengths = torch.LongTensor([len(sentence) for sentence in sentence_indices])\n",
    "        sentence_tensors = [torch.LongTensor(sentence).to(device) for sentence in  sentence_indices]\n",
    "        padded = torch.nn.utils.rnn.pad_sequence(sentence_tensors, batch_first=True)\n",
    "        lengths, perm_idx = lengths.sort(0, descending=True)\n",
    "        # perm_idx is the permutation of sentence indices as sorted by length\n",
    "        padded = padded[perm_idx]\n",
    "        return padded, lengths\n",
    "    \n",
    "    length = len(sentences)\n",
    "    # the index to separate train from test\n",
    "    split = int(length * train_test_split)\n",
    "    \n",
    "    # shuffle before splitting so test doesn't just get the alphabetically sooner sentences\n",
    "    sentences = random.sample(sentences, length)\n",
    "    \n",
    "    train_sentences = sentences[split:]\n",
    "    test_sentences = sentences[:split]\n",
    "    \n",
    "    padded_train = pad_sequence(train_sentences)\n",
    "    padded_test = pad_sequence(test_sentences)\n",
    "    \n",
    "    padded_trainset = torch.utils.data.TensorDataset(*padded_train)\n",
    "    padded_testset = torch.utils.data.TensorDataset(*padded_test)\n",
    "    \n",
    "    padded_trainloader = torch.utils.data.DataLoader(padded_trainset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    padded_testloader = torch.utils.data.DataLoader(padded_testset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    # shuffle must be false to maintain sorting by length\n",
    "    \n",
    "    return padded_trainloader, padded_testloader\n",
    "padded_en_trainloader, padded_en_testloader = padded_train_test(en_sentences, en_chartable)\n",
    "padded_jp_trainloader, padded_jp_testloader = padded_train_test(jp_sentences, jp_chartable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(trainloader, table, lr=.1, epochs=100):\n",
    "    model = Predictor(table).to(device)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.NLLLoss()\n",
    "    losses = []\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        num_losses = 0\n",
    "        for index, data in enumerate(trainloader, 0):\n",
    "            model.zero_grad()\n",
    "            padded_seq, lengths = data\n",
    "            pred = model(padded_seq, lengths)\n",
    "            \n",
    "            batch_size = padded_seq.shape[0]\n",
    "            maxlen = padded_seq.shape[1]\n",
    "            vocab_size = pred.shape[-1]\n",
    "            padded_seq_flat = padded_seq.view(batch_size*maxlen)\n",
    "            pred_flat = pred.contiguous().view(batch_size*maxlen, vocab_size)\n",
    "            \n",
    "            loss = loss_fn(pred_flat, padded_seq_flat)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.data\n",
    "            num_losses += 1\n",
    "        avg_loss = total_loss / num_losses\n",
    "        losses.append(avg_loss)\n",
    "        if (epoch + 1) % (epochs // 10) == 0:\n",
    "            print('loss at epoch {}: {}'.format(epoch+1, avg_loss))\n",
    "    print('final loss after {} epochs: {}'.format(epochs, losses[-1]))\n",
    "    return model, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save and load model\n",
    "def get_state_path(name):\n",
    "    return 'states/{}.pt'.format(name)\n",
    "def save_model(model, name):\n",
    "    torch.save(model, get_state_path(name))\n",
    "def load_model(model, name):\n",
    "    '''loads state dict into given model and returns it'''\n",
    "    model = torch.load(get_state_path(name))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jp training\n",
      "loss at epoch 10: 0.8963164687156677\n",
      "loss at epoch 20: 0.8341969847679138\n",
      "loss at epoch 30: 0.803242564201355\n",
      "loss at epoch 40: 0.7749859094619751\n",
      "loss at epoch 50: 0.7511147856712341\n",
      "loss at epoch 60: 0.730737030506134\n",
      "loss at epoch 70: 0.7125517725944519\n",
      "loss at epoch 80: 0.6955256462097168\n",
      "loss at epoch 90: 0.6789075136184692\n",
      "loss at epoch 100: 0.6623553037643433\n",
      "final loss after 100 epochs: 0.6623553037643433\n",
      "en training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\mthun\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\torch\\serialization.py:251: UserWarning: Couldn't retrieve source code for container of type Predictor. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at epoch 10: 0.2676386535167694\n",
      "loss at epoch 20: 0.18314088881015778\n",
      "loss at epoch 30: 0.1319272667169571\n",
      "loss at epoch 40: 0.10008544474840164\n",
      "loss at epoch 50: 0.07900269329547882\n",
      "loss at epoch 60: 0.06452611833810806\n",
      "loss at epoch 70: 0.05410991609096527\n",
      "loss at epoch 80: 0.04621864855289459\n",
      "loss at epoch 90: 0.03998982906341553\n",
      "loss at epoch 100: 0.034939952194690704\n",
      "final loss after 100 epochs: 0.034939952194690704\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Predictor(\n",
       "   (embedding): Embedding(3910, 64)\n",
       "   (RNN): LSTM(64, 100, batch_first=True)\n",
       "   (linear): Sequential(\n",
       "     (0): Linear(in_features=100, out_features=3910, bias=True)\n",
       "     (1): LogSoftmax()\n",
       "   )\n",
       " ), Predictor(\n",
       "   (embedding): Embedding(172, 64)\n",
       "   (RNN): LSTM(64, 100, batch_first=True)\n",
       "   (linear): Sequential(\n",
       "     (0): Linear(in_features=100, out_features=172, bias=True)\n",
       "     (1): LogSoftmax()\n",
       "   )\n",
       " ))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def initialize_models(should_train=True):\n",
    "    global jp_model, en_model\n",
    "    jp_model = Predictor(jp_chartable).to(device)\n",
    "    jp_losses = None\n",
    "    en_model = Predictor(en_chartable).to(device)\n",
    "    en_losses = None\n",
    "    if should_train:\n",
    "        print('jp training')\n",
    "        jp_model, jp_losses = train_model(padded_jp_trainloader, jp_chartable)\n",
    "        save_model(jp_model, 'jp_char_model')\n",
    "        print('en training')\n",
    "        en_model, en_losses = train_model(padded_en_trainloader, en_chartable)\n",
    "        save_model(en_model, 'en_char_model')\n",
    "    else:\n",
    "        jp_model = load_model(jp_model, 'jp_char_model')\n",
    "        en_model = load_model(en_model, 'en_char_model')\n",
    "    return jp_model, en_model\n",
    "initialize_models(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics\n",
    "def perplexity_metric(pred, actual):\n",
    "    '''\n",
    "    pred (batch, seq, vocab) logsoftmax\n",
    "    actual (batch, seq) longs\n",
    "    geometric mean of product of p(next word | previous words) for whole sentence\n",
    "    average (arithmetic mean) by batch\n",
    "    '''\n",
    "    batch_size, seq_len, vocab_size = pred.shape\n",
    "    pred = pred.cpu()\n",
    "    pred = torch.exp(pred)\n",
    "    geo_means = [] # probabilities of correct characters\n",
    "    for i in range(batch_size):\n",
    "        product = 1\n",
    "        num_factors = 0\n",
    "        curr_pred = pred[i]\n",
    "        curr_actual = actual[i]\n",
    "        for t in range(seq_len):\n",
    "            trueInd = curr_actual[t].item()\n",
    "            # the character index at this timestep\n",
    "            if trueInd != 0:\n",
    "                # we don't care how well it predicts nulls\n",
    "                predSoftmax = curr_pred[t]\n",
    "                confidence = predSoftmax[trueInd].item()\n",
    "                product *= confidence\n",
    "                num_factors += 1\n",
    "        geo_means.append(product ** (1/num_factors))\n",
    "    return sum(geo_means) / len(geo_means)\n",
    "def print_metrics(model, name, testloader, word=False):\n",
    "    loss_fn = nn.NLLLoss()\n",
    "    losses = []\n",
    "    sentence_accuracies = []\n",
    "    character_accuracies = []\n",
    "    perplexities = []\n",
    "    for index, data in enumerate(testloader, 0):\n",
    "        padded_seq, lengths = data\n",
    "        pred, maxInds = model.predict(padded_seq, lengths)\n",
    "        \n",
    "        perplexity = perplexity_metric(pred, padded_seq)\n",
    "        \n",
    "        batch_size = padded_seq.shape[0]\n",
    "        maxlen = padded_seq.shape[1]\n",
    "        vocab_size = pred.shape[-1]\n",
    "        \n",
    "        padded_seq_flat = padded_seq.view(batch_size*maxlen)\n",
    "        pred_flat = pred.contiguous().view(batch_size*maxlen, vocab_size)\n",
    "        loss = loss_fn(pred_flat, padded_seq_flat).item()\n",
    "        \n",
    "        correct_characters = torch.sum(maxInds == padded_seq).item()\n",
    "        total_characters = batch_size*maxlen\n",
    "        correct_sentences = 0\n",
    "        total_sentences = batch_size\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            if torch.all(maxInds[i] == padded_seq[i]):\n",
    "                correct_sentences += 1\n",
    "        sentence_accuracy = correct_sentences / total_sentences\n",
    "        character_accuracy = correct_characters / total_characters\n",
    "        \n",
    "        losses.append(loss)\n",
    "        sentence_accuracies.append(sentence_accuracy)\n",
    "        character_accuracies.append(character_accuracy)\n",
    "        perplexities.append(perplexity)\n",
    "    loss_avg = sum(losses) / len(losses)\n",
    "    sentence_accuracy_avg = sum(sentence_accuracies) / len(sentence_accuracies)\n",
    "    character_accuracy_avg = sum(character_accuracies) / len(character_accuracies)\n",
    "    perplexity_avg = sum(perplexities) / len(perplexities)\n",
    "    if word:\n",
    "        print('model: {}\\n\\tvalidation loss: {}\\n\\tsentence accuracy: {}\\n\\tword accuracy: {}\\n\\tperplexity: {}'.format(name, loss_avg, sentence_accuracy_avg, character_accuracy_avg, perplexity_avg))\n",
    "    else:\n",
    "        print('model: {}\\n\\tvalidation loss: {}\\n\\tsentence accuracy: {}\\n\\tcharacter accuracy: {}\\n\\tperplexity: {}'.format(name, loss_avg, sentence_accuracy_avg, character_accuracy_avg, perplexity_avg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# english word-to-word\n",
    "since the japanese model had to learn a mixture of character prediction and word prediction at the same time, let's see how the english model predicts words, and compare it to the japanese character predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mthun\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', '102nd', 'head', 'priest', ',', 'Nikko', 'TOSHIDA']\n"
     ]
    }
   ],
   "source": [
    "# tokenize sentences\n",
    "tokenized_sentences = []\n",
    "for sentence in en_sentences:\n",
    "    tokenized = nltk.word_tokenize(sentence)\n",
    "    if len(tokenized) > 0:\n",
    "        tokenized_sentences.append(tokenized)\n",
    "print(tokenized_sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43216"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordlist = []\n",
    "for sentence in tokenized_sentences:\n",
    "    for word in sentence:\n",
    "        wordlist.append(word)\n",
    "wordset = set(wordlist)\n",
    "len(wordset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### that's way too many words!\n",
    "let's limit the vocab size to 4000 to make the complexity theoretically similar to the japanese model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "of\t6995\n",
      "(\t6793\n",
      ")\t6769\n",
      "the\t5777\n",
      ",\t3457\n",
      "no\t2899\n",
      "a\t2872\n",
      "Temple\t1617\n",
      "and\t1278\n",
      "in\t1175\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_vocab_size = 4000\n",
    "# word -> frequency\n",
    "counts = {}\n",
    "for word in wordlist:\n",
    "    if word in counts:\n",
    "        counts[word] += 1\n",
    "    else:\n",
    "        counts[word] = 1\n",
    "sorted_wordset = sorted(list(wordset), key=lambda word: counts[word], reverse=True)\n",
    "for word in sorted_wordset[:10]:\n",
    "    print(word, counts[word], sep='\\t')\n",
    "vocab = set([])\n",
    "for word in sorted_wordset:\n",
    "    if len(vocab) < max_vocab_size:\n",
    "        vocab.add(word)\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "holding\n",
      "<unk>\n",
      "4002\n"
     ]
    }
   ],
   "source": [
    "# word encoding and decoding\n",
    "class WordTable:\n",
    "    def __init__(self, wordset):\n",
    "        self.wordset = frozenset(wordset)\n",
    "        self.wordlist = ['<null>', '<unk>'] + list(wordset)\n",
    "        self.vocab_size = len(self.wordlist)\n",
    "        \n",
    "        \n",
    "    def encode(self, word):\n",
    "        '''\n",
    "        expects word string or possibly nested list of word strings\n",
    "        unks out-of-vocab words\n",
    "        word(s) -> indices\n",
    "        '''\n",
    "        if type(word) == type('asdf'):\n",
    "            if word in self.wordlist:\n",
    "                return self.wordlist.index(word)\n",
    "            else:\n",
    "                # encode out-of-vocab words with unk\n",
    "                return self.wordlist.index('<unk>')\n",
    "        else:\n",
    "            words = word\n",
    "            return [self.encode(word) for word in words]\n",
    "        \n",
    "        \n",
    "    def decode(self, wordInd):\n",
    "        '''\n",
    "        expects wordInd index or possibly nested list of word indices\n",
    "        '''\n",
    "        if type(wordInd) == type(123):\n",
    "            return self.wordlist[wordInd]\n",
    "        else:\n",
    "            wordInds = wordInd\n",
    "            return [self.decode(wordInd) for wordInd in wordInds]\n",
    "wordtable = WordTable(vocab)\n",
    "print(wordtable.decode(200))\n",
    "print(wordtable.decode(wordtable.encode('why relu works')))\n",
    "print(wordtable.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "padded_word_trainloader, padded_word_testloader = padded_train_test(tokenized_sentences, wordtable, word=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training english word model\n",
      "loss at epoch 10: 0.3745356500148773\n",
      "loss at epoch 20: 0.3207111358642578\n",
      "loss at epoch 30: 0.285001277923584\n",
      "loss at epoch 40: 0.26759713888168335\n",
      "loss at epoch 50: 0.2563556432723999\n",
      "loss at epoch 60: 0.24777519702911377\n",
      "loss at epoch 70: 0.24099139869213104\n",
      "loss at epoch 80: 0.2355344444513321\n",
      "loss at epoch 90: 0.23100298643112183\n",
      "loss at epoch 100: 0.22712047398090363\n",
      "final loss after 100 epochs: 0.22712047398090363\n"
     ]
    }
   ],
   "source": [
    "print('training english word model')\n",
    "word_model, word_model_losses = train_model(padded_word_trainloader, wordtable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: jp character predictor\n",
      "\tvalidation loss: 0.6796544634160542\n",
      "\tsentence accuracy: 0.0008571428571428572\n",
      "\tcharacter accuracy: 0.9120515418737644\n",
      "\tperplexity: 0.002167849192874908\n",
      "model: en character predictor\n",
      "\tvalidation loss: 0.037989046247232525\n",
      "\tsentence accuracy: 0.2675401635401635\n",
      "\tcharacter accuracy: 0.9946749398749398\n",
      "\tperplexity: 0.6145452432555603\n",
      "model: english word to word\n",
      "\tvalidation loss: 0.3671317316946529\n",
      "\tsentence accuracy: 0.2557873977873978\n",
      "\tword accuracy: 0.9556184802851468\n",
      "\tperplexity: 0.267146103237912\n"
     ]
    }
   ],
   "source": [
    "print_metrics(jp_model, 'jp character predictor', padded_jp_testloader)\n",
    "print_metrics(en_model, 'en character predictor', padded_en_testloader)\n",
    "print_metrics(word_model, 'english word to word', padded_word_testloader, word=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  results and conclusions\n",
    "* the english character-to-character predictor vastly outperformed the japanese character-to-character predictor\n",
    "* This may be due to the higher complexity of the japanese text. There are 3912 characters in the japanese text, but only 174 in the english text.\n",
    "* Since the model uses a linear layer and softmax with dimensions equal to the number of characters, that means there is a lot more parameters for the japanese model to learn.\n",
    "* Due to the nature of the Japanese writing system, the model must effectively learn word-to-word and character-to-character prediction at the same time. This could be why it performed more poorly than the english model\n",
    "* Although the english model had to learn english spelling (which is hard) and also how words fit together, it still performed much better\n",
    "* character accuracies were pretty similar, but in all other metrics, the english model performed significantly better. Perhaps this means the japanese model only leanred some short-term patterns, but not long-term \"meaning\" to predict an entire sentence accurately.\n",
    "* sentence accuracy is near zero with the japanese model, but 25% with the english model.\n",
    "* High character accuracy despite low sentence accuracy and low perplexity shows that the japanese model is not confident in its correct predictions and does not have the ability to model language on a sentence level like the english model did.\n",
    "* despite theoretically having similar model complexity, the english word predictor model significantly out-performs the japanese character-to-character model.\n",
    "* perhaps the mixture of character and word prediction in japanese is what makes it so difficult. Or perhaps there are other factors that make it more difficult, such as japanese grammar being harder to model, or kanji having multiple meanings. Or maybe the translation is bad/inconsistent.\n",
    "* To investigate whether the grammar is causing difficulty to model japanese, a translation dataset with Japanese text transcribed into the latin alphabet with words separated by spaces would be ideal. With such a dataset, \"true\" character to character and word to word prediction can be done in japanese.\n",
    "* the english word prediction model significantly outperformed the japanese character prediction model. This shows that english is easier to model than japanese in both ways. It isn't just easier to model with character prediction, but word prediction as well. I think it is because there are 3 different writing systems in japanese and they are used and structured very differently. It is hard to model 3 writing systems at the same time.\n",
    "* the english word prediction model had a similar sentence accuracy to the english character prediction model, despite the higher complexity. This is likely due to the fact that words are ordered in a logical way, while a sequence of characters is more arbitrary. When the character prediction model predicts a sentence, it implicitly models the words from the sequence of characters and the current word being built, all in one hidden state vector. This must be difficult. If it weren't for the increased complexity of the word prediction model due to the higher vocabulary space, it might outperform the character prediction model.\n",
    "* the loss for the english character model went down do 10% of its original loss during training, but the word prediction model approximately halved its loss in the same number of epochs. This could be due to the complexity differences between the models.  \n",
    "### future research\n",
    "* use a parallel translated text transcribed to the latin alphabet. this would eliminate some of the effects of the writing system and come closer to comparing the grammars and vocabularies of the languages and their easiness to learn/model.\n",
    "* investigate different hyperparameters (such as embedding/hidden dimensions, learning rates, etc.) maybe if all of the models had a larger hidden vector size, the japanese model might outperform the english models. I kept all hyperparameters the same across all models to reduce the number of independent variables, but maybe it would be better to increase model complexity according to the complexity of the task. perhaps scale embedding dimensions linearly with vocab size?\n",
    "* compare english to other european languages to isolate the language from the writing system. Is english easier to model than italian? is spanish easier to model than italian? Here, it's hard to tell how much of the difference in performance is from the writing systems and how much is from the languages and grammars themselves."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
