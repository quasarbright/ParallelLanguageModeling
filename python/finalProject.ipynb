{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from IPython.core.debugger import set_trace\n",
    "# set device\n",
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['102世吉田日厚貫首', '1月15日：成人祭、新年祭', '1月3日：家運隆盛、商売繁盛祈願祭', '1月7日：七種粥神事', '21世紀COEプログラム']\n",
      "['the 102nd head priest, Nikko TOSHIDA', '15th January: Seijin-sai (Adult Festival), the New Year Festival', '3rd January: Prayer Festival for the prosperity of family fortunes and business', '7th January: Nanakusa-gayu shinji (a divine service for a rice porridge with seven spring herbs to insure health for the new year)', 'The 21st Century Center Of Excellence Program']\n",
      "51982\n",
      "51982\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "jp_sentences = []\n",
    "en_sentences = []\n",
    "with open('data/kyoto_lexicon.csv', 'r', encoding='utf-8') as file:\n",
    "    reader = csv.reader(file, delimiter=',')\n",
    "    # skip the header row\n",
    "    startLooking = False\n",
    "    for row in reader:\n",
    "        if startLooking:\n",
    "            jp_sentences.append(row[0])\n",
    "            en_sentences.append(row[1])\n",
    "        startLooking = True\n",
    "print(jp_sentences[:5])\n",
    "print(en_sentences[:5])\n",
    "print(len(jp_sentences))\n",
    "print(len(en_sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# character-by-character prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[108, 170], [42, 4]]\n",
      "笛\n"
     ]
    }
   ],
   "source": [
    "# encoding and decoding characters\n",
    "class CharacterTable:\n",
    "    def __init__(self, charset):\n",
    "        self.charset = charset\n",
    "        self.charset = frozenset(self.charset)\n",
    "        self.charlist = ['<null>', '<sos>', '<eos>'] + list(self.charset)\n",
    "        # it is important that null is at index 0 since padding fills with zeroes\n",
    "        self.vocab_size = len(self.charlist)\n",
    "    def encode(self, char):\n",
    "        '''convert from character to index\n",
    "        can process (nested) list of characters'''\n",
    "        if type(char) is type('asdf'):\n",
    "            # char is a string\n",
    "            return self.charlist.index(char)\n",
    "        else:\n",
    "            # char is a list of strings\n",
    "            return [self.encode(char) for char in char]\n",
    "    def decode(self, charInd):\n",
    "        '''convert from index to character\n",
    "        can process (nested) list of indices'''\n",
    "        if type(charInd) is type(22):\n",
    "            # charInd is an int\n",
    "            return self.charlist[charInd]\n",
    "        else:\n",
    "            # charInd is a list of ints\n",
    "            return [self.encode(charInd) for charInd in charInd]\n",
    "jp_chartable = CharacterTable(set(''.join(jp_sentences)))\n",
    "en_chartable = CharacterTable(set(''.join(en_sentences)))\n",
    "print(en_chartable.encode([['a', 'b'], ['c', 'd']]))\n",
    "print(jp_chartable.decode(1234))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# character-by-character prediction model\n",
    "class CharacterPredictor(nn.Module):\n",
    "    def __init__(self, chartable, embedding_dimensions=64, hidden_size=100):\n",
    "        super(CharacterPredictor, self).__init__()\n",
    "        # model constants\n",
    "        self.embedding_dimensions = embedding_dimensions\n",
    "        self.hidden_size = hidden_size\n",
    "        self.chartable = chartable\n",
    "        self.vocab_size = self.chartable.vocab_size\n",
    "        # model layers\n",
    "        self.embedding = nn.Embedding(self.vocab_size, embedding_dimensions)\n",
    "        self.RNN = nn.LSTM(\n",
    "            input_size=self.embedding_dimensions,\n",
    "            hidden_size=self.hidden_size, \n",
    "            batch_first=True\n",
    "        )\n",
    "        # linear layer for converting from hidden state to softmax\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(self.hidden_size, self.vocab_size),\n",
    "            nn.LogSoftmax(dim=-1)\n",
    "        )\n",
    "    \n",
    "    \n",
    "    def forward(self, padded_seq, lengths):\n",
    "        '''\n",
    "        predicts sequence of characters at every step\n",
    "        seq (batch, seq) padded tensor of character indices\n",
    "        returns (batch, seq, vocab) softmaxes\n",
    "        implicit teacher forcing by torch RNN\n",
    "        '''\n",
    "        seq_len = padded_seq.shape[1]\n",
    "        padded_seq_embed = self.embedding(padded_seq) # (batch, seq, embed)\n",
    "        packed_seq_embed = torch.nn.utils.rnn.pack_padded_sequence(padded_seq_embed, lengths, batch_first=True)\n",
    "        packed_hidden_states, (h_final, cell_final) = self.RNN(packed_seq_embed)\n",
    "        padded_hidden_states, input_sizes = torch.nn.utils.rnn.pad_packed_sequence(packed_hidden_states, batch_first=True, total_length=seq_len)\n",
    "        # hidden_states (batch, seq, hidden) hidden states\n",
    "        y_hat = self.linear(padded_hidden_states)\n",
    "        # y_hat (batch, seq, vocab) softmaxes\n",
    "        return y_hat\n",
    "    \n",
    "    \n",
    "    def predict(self, padded_seq, lengths):\n",
    "        pred = self.forward(padded_seq, lengths)\n",
    "        # (batch, seq, vocab)\n",
    "        maxInds = pred.max(2)[1]\n",
    "        # (batch, seq)\n",
    "        return pred, maxInds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "def padded_train_test(sentences, chartable, train_test_split=.2, batch_size=500):\n",
    "    '''\n",
    "    small train_test_split means mostly train data\n",
    "    ['hello world', ...], chartable, train_test_split -> (train data, test data) padded long tensors of character indices\n",
    "    small train_test_split means mostly train data\n",
    "    output shapes (train_size, maxlen), (test_size, maxlen)\n",
    "    '''\n",
    "    def pad_sequence(sentences):\n",
    "        '''\n",
    "        ['hello world', ...] -> train dataloader, test data loader\n",
    "        outputs 2 dataloaders containing (seqs, lens)\n",
    "        tensors are padded and sorted \n",
    "        '''\n",
    "        sentence_indices = [chartable.encode(list(sentence)) for sentence in sentences]\n",
    "        # list of list of indices\n",
    "        lengths = torch.LongTensor([len(sentence) for sentence in sentence_indices])\n",
    "        sentence_tensors = [torch.LongTensor(sentence).to(device) for sentence in  sentence_indices]\n",
    "        padded = torch.nn.utils.rnn.pad_sequence(sentence_tensors, batch_first=True)\n",
    "        lengths, perm_idx = lengths.sort(0, descending=True)\n",
    "        # perm_idx is the permutation of sentence indices as sorted by length\n",
    "        padded = padded[perm_idx]\n",
    "        return padded, lengths\n",
    "    \n",
    "    length = len(sentences)\n",
    "    # the index to separate train from test\n",
    "    split = int(length * train_test_split)\n",
    "    \n",
    "    # shuffle before splitting so test doesn't just get the alphabetically sooner sentences\n",
    "    sentences = random.sample(sentences, length)\n",
    "    \n",
    "    train_sentences = sentences[split:]\n",
    "    test_sentences = sentences[:split]\n",
    "    \n",
    "    padded_train = pad_sequence(train_sentences)\n",
    "    padded_test = pad_sequence(test_sentences)\n",
    "    \n",
    "    padded_trainset = torch.utils.data.TensorDataset(*padded_train)\n",
    "    padded_testset = torch.utils.data.TensorDataset(*padded_test)\n",
    "    \n",
    "    padded_trainloader = torch.utils.data.DataLoader(padded_trainset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    padded_testloader = torch.utils.data.DataLoader(padded_testset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    # shuffle must be false to maintain sorting by length\n",
    "    \n",
    "    return padded_trainloader, padded_testloader\n",
    "padded_en_trainloader, padded_en_testloader = padded_train_test(en_sentences, en_chartable)\n",
    "padded_jp_trainloader, padded_jp_testloader = padded_train_test(jp_sentences, jp_chartable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jp training\n",
      "loss at epoch 1: 4.9767913818359375\n",
      "final loss after 1 epochs: 4.9767913818359375\n",
      "en training\n",
      "loss at epoch 1: 2.3771841526031494\n",
      "final loss after 1 epochs: 2.3771841526031494\n"
     ]
    }
   ],
   "source": [
    "def train_char(jp=True, lr=.1, epochs=1):\n",
    "    trainloader = padded_en_trainloader\n",
    "    chartable = en_chartable\n",
    "    if jp:\n",
    "        trainloader = padded_jp_trainloader\n",
    "        chartable = jp_chartable\n",
    "    model = CharacterPredictor(chartable).to(device)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.NLLLoss()\n",
    "    losses = []\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        num_losses = 0\n",
    "        for index, data in enumerate(trainloader, 0):\n",
    "            model.zero_grad()\n",
    "            padded_seq, lengths = data\n",
    "            pred = model(padded_seq, lengths)\n",
    "            \n",
    "            batch_size = padded_seq.shape[0]\n",
    "            maxlen = padded_seq.shape[1]\n",
    "            vocab_size = pred.shape[-1]\n",
    "            padded_seq_flat = padded_seq.view(batch_size*maxlen)\n",
    "            pred_flat = pred.contiguous().view(batch_size*maxlen, vocab_size)\n",
    "            \n",
    "            loss = loss_fn(pred_flat, padded_seq_flat)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.data\n",
    "            num_losses += 1\n",
    "        avg_loss = total_loss / num_losses\n",
    "        losses.append(avg_loss)\n",
    "        print('loss at epoch {}: {}'.format(epoch+1, avg_loss))\n",
    "    print('final loss after {} epochs: {}'.format(epochs, losses[-1]))\n",
    "    return model, losses\n",
    "print('jp training')\n",
    "jp_model, jp_losses = train_char()\n",
    "print('en training')\n",
    "en_model, en_losses = train_char(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics\n",
    "def perplexity(pred, actual):\n",
    "    '''\n",
    "    pred (batch, seq, vocab) logsoftmax\n",
    "    actual (batch, seq) longs\n",
    "    geometric mean of product of p(next word | previous words) for whole sentence\n",
    "    average (arithmetic mean) by batch\n",
    "    '''\n",
    "    batch_size, seq_len, vocab_size = pred.shape\n",
    "    pred = torch.exp(pred)\n",
    "    geo_means = [] # probabilities of correct characters\n",
    "    for i in range(batch_size):\n",
    "        probs = torch.index_select(pred[i], 1, actual[i])\n",
    "        product = torch.sum(probs)\n",
    "        geo_mean = torch.pow(product, 1/seq_len)\n",
    "        geo_means.append(geo_mean)\n",
    "    ### left off here debugging zero perplexity\n",
    "    return sum(geo_means) / len(geo_means)\n",
    "def print_metrics(model, name, testloader):\n",
    "    loss_fn = nn.NLLLoss()\n",
    "    losses = []\n",
    "    sentence_accuracies = []\n",
    "    character_accuracies = []\n",
    "    for index, data in enumerate(testloader, 0):\n",
    "        padded_seq, lengths = data\n",
    "        pred, maxInds = model.predict(padded_seq, lengths)\n",
    "        perplexity(pred, padded_seq)\n",
    "        \n",
    "        batch_size = padded_seq.shape[0]\n",
    "        maxlen = padded_seq.shape[1]\n",
    "        vocab_size = pred.shape[-1]\n",
    "        \n",
    "        padded_seq_flat = padded_seq.view(batch_size*maxlen)\n",
    "        pred_flat = pred.contiguous().view(batch_size*maxlen, vocab_size)\n",
    "        loss = loss_fn(pred_flat, padded_seq_flat).data\n",
    "        \n",
    "        correct_characters = torch.sum(max_ind == padded_seq)\n",
    "        total_characters = batch_size*maxlen\n",
    "        correct_sentences = 0\n",
    "        total_sentences = batch_size\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            if torch.all(maxInds[i] == padded_seq[i]):\n",
    "                correct_sentences += 1\n",
    "        sentence_accuracy = correct_sentences / total_sentences\n",
    "        character_accuracy = correct_characters / total_characters\n",
    "        \n",
    "        losses.append(loss)\n",
    "        sentence_accuracies.append(sentence_accuracy)\n",
    "        character_accuracies.append(character_accuracy)\n",
    "    loss_avg = sum(losses) / len(losses)\n",
    "    sentence_accuracy_avg = sum(sentence_accuracies) / len(sentence_accuracies)\n",
    "    character_accuracy_avg = sum(character_accuracies) / len(character_accuracies)\n",
    "    print('model: {}, validation loss: {}, sentence accuracy: {}, character accuracy: {}'.format(name, loss_avg, sentence_accuracy_avg, character_accuracy_avg))\n",
    "print_metrics(jp_model, 'jp character predictor', padded_jp_testloader)\n",
    "print_metrics(en_model, 'en character predictor', padded_en_testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1 + 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
